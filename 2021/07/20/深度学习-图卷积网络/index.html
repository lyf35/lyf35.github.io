<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#222">
  <link rel="manifest" href="/images/site.webmanifest">
  <meta name="msapplication-config" content="/images/browserconfig.xml">
  <meta name="msvalidate.01" content="<meta name="msvalidate.01" content="5D3796A5DDB32CC875380613FB613833" />">
  <meta name="baidu-site-verification" content="<meta name="baidu-site-verification" content="kewHZtFYQk" />">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Helvetica:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  
  <link rel="stylesheet" href="/lib/animate-css/animate.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-flat-top.min.css">
  <script src="//cdn.jsdelivr.net/npm/pace-js@1/pace.min.js"></script>

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"lyf35.github.io","root":"/","scheme":"Pisces","version":"8.0.0-rc.5","exturl":true,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":{"gitalk":{"order":-2}},"activeClass":"gitalk"},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"path":"search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>

  <meta name="description" content="简介 目前在各类文章中所提到的图卷积网络（Graph Convolutional Network）有两层含义：广义的GCN指的是图神经网络中的一个子类，泛指使用了图卷积运算的网络结构（例如综述文章Deep Learning on Graphs: A Survey中，GCN一词的含义）；而狭义的GCN则特指Kipf和Welling在文章SEMI-SUPERVISED CLASSIFICATION W">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习-图卷积网络">
<meta property="og:url" content="http://lyf35.github.io/2021/07/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/index.html">
<meta property="og:site_name" content="Yufei Luo&#39;s Blog">
<meta property="og:description" content="简介 目前在各类文章中所提到的图卷积网络（Graph Convolutional Network）有两层含义：广义的GCN指的是图神经网络中的一个子类，泛指使用了图卷积运算的网络结构（例如综述文章Deep Learning on Graphs: A Survey中，GCN一词的含义）；而狭义的GCN则特指Kipf和Welling在文章SEMI-SUPERVISED CLASSIFICATION W">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/lyf35/blog_figures/main/img/20210801215241.png">
<meta property="og:image" content="https://raw.githubusercontent.com/lyf35/blog_figures/main/img/20210801215229.png">
<meta property="article:published_time" content="2021-07-20T11:35:54.000Z">
<meta property="article:modified_time" content="2021-07-22T15:32:41.000Z">
<meta property="article:author" content="Yufei Luo">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="图神经网络">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/lyf35/blog_figures/main/img/20210801215241.png">

<link rel="canonical" href="http://lyf35.github.io/2021/07/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/">


<script data-pjax class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>深度学习-图卷积网络 | Yufei Luo's Blog</title>
  






  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
 <!--   <div class="headband"></div>-->

    <main class="main">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader">
        <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Yufei Luo's Blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">But I was so much older then, I am younger than that now.</p>
      <img class="custom-logo-image" src="/images/logo.png" alt="Yufei Luo's Blog">
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-计算机基础">

    <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80" rel="section"><i class="fa fa-tags fa-fw"></i>计算机基础</a>

  </li>
        <li class="menu-item menu-item-机器学习">

    <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0" rel="section"><i class="fa fa-tags fa-fw"></i>机器学习</a>

  </li>
        <li class="menu-item menu-item-深度学习">

    <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0" rel="section"><i class="fa fa-tags fa-fw"></i>深度学习</a>

  </li>
        <li class="menu-item menu-item-工程实践">

    <a href="/categories/%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5" rel="section"><i class="fa fa-tags fa-fw"></i>工程实践</a>

  </li>
        <li class="menu-item menu-item-论文笔记">

    <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0" rel="section"><i class="fa fa-tags fa-fw"></i>论文笔记</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404" rel="section"><i class="fa fa-heartbeat fa-fw"></i>公益 404</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <section class="post-toc-wrap sidebar-panel">
          <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AE%80%E4%BB%8B"><span class="nav-number">1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E9%A2%91%E5%9F%9F%E7%9A%84%E5%9B%BE%E5%8D%B7%E7%A7%AF"><span class="nav-number">2.</span> <span class="nav-text">基于频域的图卷积</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8E%9F%E7%90%86"><span class="nav-number">2.1.</span> <span class="nav-text">原理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C"><span class="nav-number">2.1.1.</span> <span class="nav-text">卷积操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E7%9F%A9%E9%98%B5"><span class="nav-number">2.1.2.</span> <span class="nav-text">拉普拉斯矩阵</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2"><span class="nav-number">2.1.3.</span> <span class="nav-text">图傅里叶变换</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%E5%8D%B7%E7%A7%AF"><span class="nav-number">2.1.4.</span> <span class="nav-text">图卷积</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%87%A0%E7%A7%8D%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.2.</span> <span class="nav-text">几种经典模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#scnn"><span class="nav-number">2.2.1.</span> <span class="nav-text">SCNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#chebnet"><span class="nav-number">2.2.2.</span> <span class="nav-text">ChebNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gcn"><span class="nav-number">2.2.3.</span> <span class="nav-text">GCN</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E7%A9%BA%E5%9F%9F%E7%9A%84%E5%9B%BE%E5%8D%B7%E7%A7%AF"><span class="nav-number">3.</span> <span class="nav-text">基于空域的图卷积</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8E%9F%E7%90%86-1"><span class="nav-number">3.1.</span> <span class="nav-text">原理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E4%B8%BE%E4%BE%8B"><span class="nav-number">3.2.</span> <span class="nav-text">模型举例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#nn4g"><span class="nav-number">3.2.1.</span> <span class="nav-text">NN4G</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dcnn"><span class="nav-number">3.2.2.</span> <span class="nav-text">DCNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#neural-fps"><span class="nav-number">3.2.3.</span> <span class="nav-text">Neural FPs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#graphsage"><span class="nav-number">3.2.4.</span> <span class="nav-text">GraphSAGE</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gin"><span class="nav-number">3.2.5.</span> <span class="nav-text">GIN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gat"><span class="nav-number">3.2.6.</span> <span class="nav-text">GAT</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%80%9A%E7%94%A8%E6%A1%86%E6%9E%B6"><span class="nav-number">4.</span> <span class="nav-text">通用框架</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#mpnn"><span class="nav-number">4.1.</span> <span class="nav-text">MPNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#gn"><span class="nav-number">4.2.</span> <span class="nav-text">GN</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B"><span class="nav-number">5.</span> <span class="nav-text">代码示例</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">6.</span> <span class="nav-text">参考</span></a></li></ol></div>
      </section>
      <!--/noindex-->

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Yufei Luo"
      src="/images/avatar.jpeg">
  <p class="site-author-name" itemprop="name">Yufei Luo</p>
  <div class="site-description" itemprop="description">哪怕什么真理无穷，进一寸有进一寸的欢喜</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
        
          <span class="site-state-item-count">90</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories">
          
        <span class="site-state-item-count">24</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags">
          
        <span class="site-state-item-count">30</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2x5ZjM1" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;lyf35"><i class="fab fa-github fa-fw"></i>GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOmx5ZjEyMzQwMDAwMDBAMTYzLmNvbQ==" title="E-Mail → mailto:lyf1234000000@163.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</span>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></span>
  </div>



      </section>
        <div class="back-to-top animated">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </header>

      
  <div class="reading-progress-bar"></div>

  <span class="exturl github-corner" data-url="aHR0cHM6Ly9naXRodWIuY29tL2x5ZjM1" title="Follow me on GitHub" aria-label="Follow me on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></span>

<noscript>
  <div id="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


      <div class="main-inner">
        

        <div class="content post posts-expand">
          

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://lyf35.github.io/2021/07/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="Yufei Luo">
      <meta itemprop="description" content="哪怕什么真理无穷，进一寸有进一寸的欢喜">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yufei Luo's Blog">
    </span>

    
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深度学习-图卷积网络
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-07-20 19:35:54" itemprop="dateCreated datePublished" datetime="2021-07-20T19:35:54+08:00">2021-07-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-07-22 23:32:41" itemprop="dateModified" datetime="2021-07-22T23:32:41+08:00">2021-07-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%90%86%E8%AE%BA%E7%9F%A5%E8%AF%86/" itemprop="url" rel="index"><span itemprop="name">理论知识</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%90%86%E8%AE%BA%E7%9F%A5%E8%AF%86/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          
            <span id="/2021/07/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" class="post-meta-item leancloud_visitors" data-flag-title="深度学习-图卷积网络" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>22k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>20 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="简介">简介</h1>
<p>目前在各类文章中所提到的图卷积网络（Graph Convolutional Network）有两层含义：广义的GCN指的是图神经网络中的一个子类，泛指使用了图卷积运算的网络结构（例如综述文章Deep Learning on Graphs: A Survey中，GCN一词的含义）；而狭义的GCN则特指Kipf和Welling在文章<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MDkuMDI5MDcucGRm">SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS<i class="fa fa-external-link-alt"></i></span>中提出的GCN网络结构。本文中如果无特殊说明，则使用的是泛指的概念。</p>
<p>“图卷积”指的是将欧式数据（如图像、时间序列这类规则的数据）上的卷积操作进一步泛化到非欧式的图结构数据中。它的主要思想是通过聚合结点自己及其相邻结点的特征，从而得到结点的表示。下图展示了图卷积网络是如何进行计算的：</p>
<p><img data-src="https://raw.githubusercontent.com/lyf35/blog_figures/main/img/20210801215241.png" alt="Multi-layer Graph Convolutional Network (GCN) with first-order filters." style="zoom:67%;" /></p>
<span id="more"></span>
<p>图结构中的卷积运算可分为两类，基于频域（Spectral）的方法和基于空域（Spatial）的方法。基于频域方法的理论来源于图结构的信号处理，在频域空间定义图的卷积操作；而基于空域的方法则依据图的拓扑结构，直接在图结构上定义卷积运算。在这两大类方法中又有许多不同种类的变体。</p>
<h1 id="基于频域的图卷积">基于频域的图卷积</h1>
<h2 id="原理">原理</h2>
<h3 id="卷积操作">卷积操作</h3>
<p>根据卷积定理，两个信号<span class="math inline">\(\boldsymbol{g}\)</span>和<span class="math inline">\(\boldsymbol{x}\)</span>在空域（或者说时域）的卷积傅里叶变换等于这两个信号在频域中的傅里叶变换的乘积，即： <span class="math display">\[
\boldsymbol{g} \star \boldsymbol{x} = \mathscr{F}^{-1}(\mathscr{F}(\boldsymbol{g})\odot\mathscr{F}(\boldsymbol{x}))
\]</span> 其中，<span class="math inline">\(\mathscr{F}(\cdot)\)</span>指的是傅里叶变换，而<span class="math inline">\(\mathscr{F}^{-1}(\cdot)\)</span>指的是傅里叶逆变换。通常，我们将<span class="math inline">\(\boldsymbol{x}\)</span>称为输入信号，<span class="math inline">\(\boldsymbol{g}\)</span>称为卷积核或者滤波器。</p>
<p>上式的意思也就是将空域上的信号转换为频域上的信号，然后将频域上的信号相乘。对相乘后的结果做傅里叶反变换，便得到了空域上卷积运算的结果。</p>
<p>由于经典的卷积操作具有序列有序性和维数不变性的限制，使得经典卷积难以处理图数据。但是当我们把数据从空域转换到频域，在频域处理数据时，只需要将每个频域的分量放大或者缩小就可以了，不需要考虑信号在空域上存在的问题，这个就是频域图卷积的巧妙之处。</p>
<blockquote>
<p>备注：此处对于傅里叶变换、卷积等概念的解释比较简略，更详细的解释可以参考下面的资料</p>
<ul>
<li>对卷积的解释：<span class="exturl" data-url="aHR0cHM6Ly93d3cuemhpaHUuY29tL3F1ZXN0aW9uLzIyMjk4MzUy">如何通俗易懂地解释卷积？ - 知乎 (zhihu.com)<i class="fa fa-external-link-alt"></i></span></li>
<li>对傅里叶变换的解释：<span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8xOTc2MzM1OA==">傅里叶分析之掐死教程（完整版）更新于2014.06.06 - 知乎 (zhihu.com)<i class="fa fa-external-link-alt"></i></span></li>
</ul>
</blockquote>
<h3 id="拉普拉斯矩阵">拉普拉斯矩阵</h3>
<p>在介绍图的傅里叶变换之前，我们需要先介绍一些关于图的拉普拉斯矩阵的概念。图的拉普拉斯矩阵是由拉普拉斯算子推广到图而得，为了简单起见，接下来我们以一维的情况为例，来推导拉普拉斯矩阵的定义。</p>
<p>对于多元函数<span class="math inline">\(f(x_1,\dots,x_n)\)</span>，它的拉普拉斯算子定义如下： <span class="math display">\[
\Delta f =\sum_{i=1}^{n}\frac{\partial^2 f}{\partial x_i^2}
\]</span> 记<span class="math inline">\(x_{i+1}=x_i+\Delta x\)</span>，<span class="math inline">\(x_{i-1}=x_i-\Delta x\)</span>这一表达式的离散化形式为： <span class="math display">\[
\Delta f \approx \sum_{i=1}^{n} \frac{f(x_1,\dots,x_{i+1},\dots,x_n)+f(x_1,\dots,x_{i-1},\dots,x_n)-2f(x_1,\dots,x_n)}{\Delta (x)^2}
\]</span> 这一结果相当于是<span class="math inline">\((x_1,\dots,x_n)\)</span>的<span class="math inline">\(2n\)</span>个相邻点处的函数值之和与<span class="math inline">\((x_1,\dots,x_n)\)</span>出的函数值乘<span class="math inline">\(2n\)</span>后的差值。如果我们记<span class="math inline">\(N(x_1,\dots,x_n)\)</span>为<span class="math display">\[(x_1,\dots,x_n)\]</span>邻居结点的集合，那么拉普拉斯算子的计算公式也可以写为如下格式： <span class="math display">\[
\Delta f = \sum_{(x_1&#39;,\dots,x_n&#39;)\in N(x_1,\dots,x_n)}(f(x_1&#39;,\dots,x_n&#39;)-f(x_1,\dots,x_n))
\]</span> 这一表达式可以进一步推广到图结构中，如果将节点<span class="math inline">\(i\)</span>的邻居节点的集合记为<span class="math inline">\(N_i\)</span>，节点<span class="math inline">\(i\)</span>的值看作是函数值，那么节点<span class="math inline">\(i\)</span>的拉普拉斯算子可以定义为： <span class="math display">\[
\Delta f_i=\sum_{j\in N_i}(f_i-f_j)
\]</span> 由于图的边可以带有权重，因此在这一计算公式中也可以加上每条边对应的权重： <span class="math display">\[
\Delta f_i=\sum_{j\in N_i}w_{ij}(f_i-f_j)
\]</span> 如果进一步设定当<span class="math inline">\(j\)</span>不是<span class="math inline">\(i\)</span>的邻居时，<span class="math inline">\(w_{ij}=0\)</span>，那么上式也可以写为： <span class="math display">\[
\Delta f_i=\sum_{j\in V}w_{ij}(f_i-f_j)=\sum_{j\in V}w_{ij}f_i-\sum_{j\in V}w_{ij}f_j=d_i f_i-\boldsymbol{w}_i \boldsymbol{f}
\]</span> 其中，<span class="math inline">\(V\)</span>代表图中所有结点的集合，<span class="math inline">\(\boldsymbol{f}\)</span>为所有结点的值构成的列向量，<span class="math inline">\(d_i\)</span>代表结点<span class="math inline">\(i\)</span>加权后的度，<span class="math inline">\(\boldsymbol{w}_i\)</span>代表加权邻接矩阵的第<span class="math inline">\(i\)</span>行对应的向量。</p>
<p>由于图中所有的结点都做的是这样类似的运算，这一表达式也可以写成矩阵形式，其中<span class="math inline">\(\boldsymbol{D}\)</span>为加权度矩阵，<span class="math inline">\(\boldsymbol{A}\)</span>为加权的邻接矩阵： <span class="math display">\[
\Delta \boldsymbol{f}=(\boldsymbol{D}-\boldsymbol{A})\boldsymbol{f}
\]</span> 而拉普拉斯矩阵也被定义为加权度矩阵与加权邻接矩阵的差，即<span class="math inline">\(\boldsymbol{L}=\boldsymbol{D}-\boldsymbol{A}\)</span>​​。由于<span class="math inline">\(\boldsymbol{A}\)</span>​​和<span class="math inline">\(\boldsymbol{D}\)</span>​​都是对称矩阵，因此拉普拉斯矩阵<span class="math inline">\(\boldsymbol{L}\)</span>​​也为对称矩阵。而根据拉普拉斯算子的定义，拉普拉斯矩阵实际上也代表了图的二阶导数。</p>
<p>拉普拉斯矩阵具有如下性质：</p>
<ul>
<li><p>对于任意向量<span class="math inline">\(\boldsymbol{f}=(f_1,\dots,f_n)\)</span>，有： <span class="math display">\[
\boldsymbol{f}^T \boldsymbol{L} \boldsymbol{f}=\frac{1}{2}\sum_{i=1}^n \sum_{j=1}^n w_{ij}(f_i-f_j)^2
\]</span></p></li>
<li><p>由上一结论可得，拉普拉斯矩阵为对称半正定矩阵</p></li>
<li><p>拉普拉斯矩阵的最小特征值为0，其对应的特征向量为所有分量为1的常向量</p></li>
<li><p>拉普拉斯矩阵有<span class="math inline">\(n\)</span>个非负实数特征值</p></li>
</ul>
<p>由于拉普拉斯矩阵是对称半正定矩阵，因此它一定有<span class="math inline">\(n\)</span>个相互正交的特征向量，这些特征向量构成了<span class="math inline">\(n\)</span>维空间的一组标准正交基。因此，拉普拉斯矩阵的谱分解相当于如下形式： <span class="math display">\[
\boldsymbol{L}=\boldsymbol{U}\boldsymbol{\Lambda}\boldsymbol{U}^{-1}
\]</span> 其中<span class="math inline">\(\boldsymbol{U}=(\boldsymbol{u}_1,\dots,\boldsymbol{u}_n)\)</span>​，<span class="math inline">\(\boldsymbol{u}_i\in \mathbb{R}^n\)</span>​；<span class="math inline">\(\boldsymbol{\Lambda}\)</span>​是由<span class="math inline">\(n\)</span>​个特征值组成的对角矩阵，即<span class="math inline">\(\boldsymbol{\Lambda}=\begin{pmatrix}\lambda_1 \\ &amp; \ddots \\ &amp; &amp; \lambda_n \end{pmatrix}\)</span>​。​</p>
<p>对拉普拉斯矩阵进行归一化即可得到归一化的拉普拉斯矩阵。通常有两种形式的归一化：</p>
<ol type="1">
<li>对称归一化：<span class="math inline">\(\boldsymbol{L}_{\text{sym}}=\boldsymbol{D}^{-\frac{1}{2}}\boldsymbol{L}\boldsymbol{D}^{-\frac{1}{2}}=\boldsymbol{I}-\boldsymbol{D}^{-\frac{1}{2}}\boldsymbol{A}\boldsymbol{D}^{-\frac{1}{2}}\)</span>​。我们设<span class="math inline">\(\boldsymbol{D}\)</span>​中的元素为<span class="math inline">\(d_{ij}\)</span>​，<span class="math inline">\(\boldsymbol{L}\)</span>​中的元素为<span class="math inline">\(l_{ij}\)</span>​，那么<span class="math inline">\(\boldsymbol{L}_{\text{sym}}\)</span>​中的元素<span class="math inline">\(l_{ij}&#39;=\frac{l_{ij}}{\sqrt{d_{ii}d_{jj}}}\)</span>​。</li>
<li>随机漫步归一化：<span class="math inline">\(\boldsymbol{L}_{\text{rw}}=\boldsymbol{D}^{-1}\boldsymbol{L}=\boldsymbol{I}-\boldsymbol{D}^{-1}\boldsymbol{A}\)</span>​，其中位置<span class="math inline">\(ij\)</span>​上的元素<span class="math inline">\(l_{ij}&#39;&#39;=\frac{l_{ij}}{d_{ii}}\)</span>​。</li>
</ol>
<h3 id="图傅里叶变换">图傅里叶变换</h3>
<p>图上的信号一般可以表达为一个矩阵或向量。为了简单起见，在下面的推导过程中使用向量来进行推导，即每个结点上的信号是一个数值。这样推导得到的公式也可以直接推广到结点信号为一个向量的情况，只需要对向量的每个元素都采取类似的操作即可。假设有<span class="math inline">\(n\)</span>个结点，则图上的信号可以被记为向量<span class="math inline">\(\boldsymbol{x}=({x}_1, \dots, {x}_n)^T\)</span>，其中<span class="math inline">\({x}_i\)</span>代表每个结点上的信号。</p>
<p>为了在图结构数据上做卷积操作，我们需要在图上定义傅里叶变换。由于傅里叶变换的本质是把任意一个函数表示成若干个正交基函数的线性组合，因此如果要对图上的信号<span class="math inline">\(x_i\)</span>进行傅里叶变换，则需要找到一组正交基，通过它们的线性组合来表达<span class="math inline">\(x_i\)</span>。上文介绍的拉普拉斯矩阵的特征向量是一组正交基，通过使用它，我们可以定义图上的傅里叶变换与逆变换： <span class="math display">\[
\boldsymbol{\hat{x}} =\boldsymbol{U}^T \boldsymbol{x}\\
\boldsymbol{x} =\boldsymbol{U}\boldsymbol{\hat{x}} 
\]</span></p>
<p>其中，<span class="math inline">\(\boldsymbol{\hat{x}}=(\hat{x}_1,\dots,\hat{x}_n)^T\)</span>代表每个基向量对应的系数，即傅里叶变换之后频域上的信号。</p>
<blockquote>
<p>备注：此处需要说明的一点是，并不是简单地因为拉普拉斯矩阵的特征向量正交而选择它。在经典的傅里叶变换中，它的基函数是拉普拉斯算子的本征函数（本征值和本征函数的概念可以参考<span class="exturl" data-url="aHR0cHM6Ly93d3cuemhpaHUuY29tL3F1ZXN0aW9uLzUyOTkyMjE5">如何理解本征值? - 知乎 (zhihu.com)<i class="fa fa-external-link-alt"></i></span>）。又因为拉普拉斯矩阵就是图上的拉普拉斯算子，因此图傅里叶变换的基函数即为图拉普拉斯矩阵的特征向量。</p>
</blockquote>
<p>与经典傅里叶变换相比，拉普拉斯矩阵的特征值就类似于其中的频率，而特征向量就类似于其中的基函数（即正弦函数）。低特征值对应的特征向量变换比较平滑，而高特征值对应的特征向量变换比较剧烈，相当于低频和高频基函数。</p>
<h3 id="图卷积">图卷积</h3>
<p>基于此，我们便可得到图卷积的计算公式： <span class="math display">\[
\begin{aligned}
\boldsymbol{g} \star \boldsymbol{x} =&amp; \mathscr{F}^{-1}(\mathscr{F}(\boldsymbol{g})\odot\mathscr{F}(\boldsymbol{x})) \\
=&amp;\boldsymbol{U}(\boldsymbol{U}^T\boldsymbol{g}\odot \boldsymbol{U}^T\boldsymbol{x})
\end{aligned}
\]</span> 其中<span class="math inline">\(\odot\)</span>符号代表逐元素相乘。</p>
<p>通常我们不用关心空域上的滤波器信号<span class="math inline">\(\boldsymbol{g}\)</span>​​​，只关心频域内的情况。因此可令<span class="math inline">\(\boldsymbol{g}_{\theta}=\text{diag}(\boldsymbol{U}^T \boldsymbol{g})=\begin{pmatrix} \theta_1 \\ &amp; \ddots \\ &amp; &amp; \theta_n \end{pmatrix}\)</span>​​​，则上述公式等价于： <span class="math display">\[
\boldsymbol{g} \star \boldsymbol{x} = \boldsymbol{U}\boldsymbol{g}_{\theta} \boldsymbol{U}^T\boldsymbol{x}
\]</span> 所有的频域图卷积运算都是基于上述公式的，只是对滤波器<span class="math inline">\(\boldsymbol{g}\)</span>​​​做了不同的处理。这个公式的含义相当于，通过卷积核信号将原信号不同的分量进行放大或者缩小。</p>
<h2 id="几种经典模型">几种经典模型</h2>
<h3 id="scnn">SCNN</h3>
<p>SCNN模型来源于文章<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzEzMTIuNjIwMy5wZGY=">Spectral Networks and Deep Locally Connected Networks on Graphs<i class="fa fa-external-link-alt"></i></span>，它是第一个将图卷积理论用于实践的模型，它的思路便是学习卷积核<span class="math inline">\(\boldsymbol{g}_{\theta}\)</span>​​的参数。我们设图中每个结点上的特征为一个元素，此时SCNN每一个图卷积层的输出如下： <span class="math display">\[
\boldsymbol{x}_{l+1}=f\left(\boldsymbol{U}\boldsymbol{g}_{\theta}\boldsymbol{U}^{T}\boldsymbol{x}_l\right)
\]</span> 其中<span class="math inline">\(f\)</span>代表激活函数，其余符号的含义同上文。而对于结点的特征为向量的情况，只需要对向量的每个元素都使用上面的公式即可。</p>
<p>这种方法也存在一些缺点，例如模型的参数复杂度较大，当结点数较多的时候容易过拟合；每一次前向传播的计算以及拉普拉斯矩阵特征分解的过程比较耗时等。此外更重要的一点是，这种方法不具有局部连接的性质，这是因为在矩阵运算<span class="math inline">\(\boldsymbol{U}\boldsymbol{g}_{\theta}\boldsymbol{U}^{T}\)</span>​​的结果中，每个元素都为非0值。因此，它实际上相当于是一个全连接的卷积核。</p>
<h3 id="chebnet">ChebNet</h3>
<p>ChebNet模型来源于文章<span class="exturl" data-url="aHR0cHM6Ly9wYXBlcnMubmlwcy5jYy9wYXBlci8yMDE2L2ZpbGUvMDRkZjRkNDM0ZDQ4MWM1YmI3MjNiZTFiNmRmMWVlNjUtUGFwZXIucGRm">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering (nips.cc)<i class="fa fa-external-link-alt"></i></span>，它使用Chebyshev多项式来构造卷积核，从而解决了SCNN存在的一些问题。</p>
<p>这种方法认为，可以用多项式来作为卷积核函数的近似。通过使用谱分解得到的对角矩阵<span class="math inline">\(\boldsymbol{\Lambda}\)</span>，可以构造如下形式的多项式卷积核： <span class="math display">\[
\boldsymbol{g}_{\theta}=\sum_{k=0}^{K-1}\theta_k \boldsymbol{\Lambda}^k
\]</span> 其中，<span class="math inline">\(\theta_k\)</span>​为多项式<span class="math inline">\(k\)</span>​​次项所对应的系数。而使用这种表达式，我们进一步可得： <span class="math display">\[
\begin{aligned}
\boldsymbol{U}\boldsymbol{g}_{\theta}\boldsymbol{U}^{T}= &amp; \boldsymbol{U}\sum_{k=0}^{K-1}\theta_k \boldsymbol{\Lambda}^k\boldsymbol{U}^{T} \\
=&amp; \sum_{k=0}^{K-1}\theta_k \boldsymbol{U}\boldsymbol{\Lambda}^k\boldsymbol{U}^{T} \\
=&amp; \sum_{k=0}^{K-1}\theta_k \boldsymbol{L}^k
\end{aligned}
\]</span> 根据拉普拉斯矩阵的性质，对于<span class="math inline">\(\boldsymbol{L}^k\)</span>的任意元素<span class="math inline">\(l_{ij}\)</span>，数值为0则代表通过<span class="math inline">\(k\)</span>条边结点<span class="math inline">\(i,j\)</span>无法到达，反之则代表可以到达。因此，通过使用多项式作为卷积核，便实现了卷积核的局部性。而且这种方式避免了对拉普拉斯矩阵做特征分解。但是由于要计算<span class="math inline">\(\boldsymbol{L}^k\)</span>，因此计算复杂度仍然较高。</p>
<p>由于<span class="math inline">\(\boldsymbol{L}\)</span>为稀疏矩阵，因此如果使用Chebyshev多项式，以递归的方式计算，则可以进一步地降低计算复杂度。由于Chebyshev多项式的定义域为<span class="math inline">\([-1,1]\)</span>，我们需要使用规约的对角矩阵<span class="math inline">\(\tilde{\boldsymbol{\Lambda}}=2\boldsymbol{\Lambda}/\lambda_{\text{max}}-\boldsymbol{I}\)</span>来构造多项式。即： <span class="math display">\[
\begin{aligned}
T_0(\tilde{\boldsymbol{\Lambda}})=&amp;\boldsymbol{I} \\
T_1(\tilde{\boldsymbol{\Lambda}})=&amp;\tilde{\boldsymbol{\Lambda}} \\
T_{n+1}(\tilde{\boldsymbol{\Lambda}})=&amp;2\tilde{\boldsymbol{\Lambda}} T_n(\tilde{\boldsymbol{\Lambda}})-T_{n-1}(\tilde{\boldsymbol{\Lambda}})
\end{aligned}
\]</span> 此时，<span class="math inline">\(\boldsymbol{g}_{\theta}=\sum_{k=0}^{K-1}\theta_k T_k(\tilde{\boldsymbol{\Lambda}})\)</span>​，而相应地，也有如下表达式成立： <span class="math display">\[
\begin{aligned}
\boldsymbol{U}\boldsymbol{g}_{\theta}\boldsymbol{U}^{T}= &amp; \boldsymbol{U}\sum_{k=0}^{K-1}\theta_k T_k(\tilde{\boldsymbol{\Lambda}})\boldsymbol{U}^{T} \\
=&amp; \sum_{k=0}^{K-1}\theta_k T_k(\boldsymbol{U}\tilde{\boldsymbol{\Lambda}}\boldsymbol{U}^{T}) \\
=&amp; \sum_{k=0}^{K-1}\theta_k T_k(\tilde{\boldsymbol{L}})
\end{aligned}
\]</span> 其中，<span class="math inline">\(\tilde{\boldsymbol{L}}=2\boldsymbol{L}/\lambda_{\text{max}}-\boldsymbol{I}\)</span>​代表规约的拉普拉斯矩阵。因此无需计算<span class="math inline">\(\tilde{\boldsymbol{\Lambda}}\)</span>​的Chebyshev多项式，只需计算<span class="math inline">\(\tilde{\boldsymbol{L}}\)</span>​​的Chebyshev多项式即可。</p>
<p>通过上述推导，我们可以得到ChebNet最终的图卷积计算公式： <span class="math display">\[
\boldsymbol{g} \star \boldsymbol{x} = \sum_{k=0}^{K-1}\theta_k T_k(\tilde{\boldsymbol{L}})\boldsymbol{x}
\]</span> 每个图卷积层的输出为： <span class="math display">\[
\boldsymbol{x}_{l+1}=f\left(\sum_{k=0}^{K-1}\theta_k T_k(\tilde{\boldsymbol{L}})\boldsymbol{x}_{l}\right)
\]</span> ChebNet的卷积核可学习参数只有<span class="math inline">\(k\)</span>个，而且无需再对拉普拉斯矩阵做特征分解。同时卷积核具有严格的空间局部性，<span class="math inline">\(k\)</span>的值就相当于是卷积核的感受野半径，即<span class="math inline">\(k\)</span>​阶近邻结点。</p>
<h3 id="gcn">GCN</h3>
<p>GCN来源于文章<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MDkuMDI5MDd2NC5wZGY=">Semi-Supervised Classification with Graph Convolutional Networks<i class="fa fa-external-link-alt"></i></span>。GCN的思想是只取ChebNet中的二阶近似，此时的卷积公式变为： <span class="math display">\[
\boldsymbol{g} \star \boldsymbol{x} \approx \theta_0 \boldsymbol{x}+\theta_1(\boldsymbol{L}-\boldsymbol{I})\boldsymbol{x} = \theta_0 \boldsymbol{x}-\theta_1\boldsymbol{D}^{-\frac{1}{2}}\boldsymbol{A}\boldsymbol{D}^{-\frac{1}{2}}\boldsymbol{x}
\]</span> 而如果我们做进一步的近似，令<span class="math inline">\(\theta=\theta_0=-\theta_1\)</span>​，也就是只留下一个可学习的参数，则卷积公式可进一步被近似为： <span class="math display">\[
\boldsymbol{g} \star \boldsymbol{x} \approx \theta (\boldsymbol{I}+\boldsymbol{D}^{-\frac{1}{2}}\boldsymbol{A}\boldsymbol{D}^{-\frac{1}{2}})\boldsymbol{x}
\]</span> 由于矩阵<span class="math inline">\(\boldsymbol{I}+\boldsymbol{D}^{-\frac{1}{2}}\boldsymbol{A}\boldsymbol{D}^{-\frac{1}{2}}\)</span>​​​的特征值取值范围在区间<span class="math inline">\([0,2]\)</span>​​​​内，如果这样的多层操作叠加，将会导致计算过程中的数值不稳定，出现梯度消失或者梯度爆炸的现象。为了缓解这一现象，引入了重正则化（Renormalization）技巧，即： <span class="math display">\[
\boldsymbol{I}+\boldsymbol{D}^{-\frac{1}{2}}\boldsymbol{A}\boldsymbol{D}^{-\frac{1}{2}} \rightarrow \tilde{\boldsymbol{D}}^{-\frac{1}{2}}\tilde{\boldsymbol{A}}\tilde{\boldsymbol{D}}^{-\frac{1}{2}}
\]</span> 其中，<span class="math inline">\(\tilde{\boldsymbol{A}}=\boldsymbol{A}+\boldsymbol{I}\)</span>​​，而<span class="math inline">\(\tilde{\boldsymbol{D}}_{ii}=\sum_{j}\tilde{\boldsymbol{A}}_{ij}\)</span>​​。​</p>
<p>如果每个结点的特征是个多维的向量，也同时使用多个图卷积核，那么GCN的卷积运算还可以写成如下的矩阵形式： <span class="math display">\[
\boldsymbol{Z}= \tilde{\boldsymbol{D}}^{-\frac{1}{2}}\tilde{\boldsymbol{A}}\tilde{\boldsymbol{D}}^{-\frac{1}{2}}\boldsymbol{X}\boldsymbol{\Theta}
\]</span> 其中<span class="math inline">\(\boldsymbol{X}\in \mathbb{R}^{N\times C}\)</span>代表卷积操作的输入信号矩阵，<span class="math inline">\(\boldsymbol{\Theta}\in \mathbb{R}^{C\times F}\)</span>代表图卷积操作的参数矩阵，<span class="math inline">\(\boldsymbol{Z}\in \mathbb{R}^{N\times F}\)</span>代表卷积后的信号矩阵。</p>
<p>因此，我们可以得到图卷积层的输出为： <span class="math display">\[
\boldsymbol{X}_{l+1}=f\left(\tilde{\boldsymbol{D}}^{-\frac{1}{2}}\tilde{\boldsymbol{A}}\tilde{\boldsymbol{D}}^{-\frac{1}{2}}\boldsymbol{X}_{l}\boldsymbol{\Theta}\right)
\]</span> GCN极大地减少了参数量，使得模型的复杂程度变低。但是相应地，模型的表达能力也相应减弱，可能难以处理复杂的任务。</p>
<h1 id="基于空域的图卷积">基于空域的图卷积</h1>
<h2 id="原理-1">原理</h2>
<p>频域的图卷积操作具有一些局限性，它仅适用于无向图，要求模型训练期间图结构不能变化，而且在规模较大的图上计算效率较低。基于空域的图卷积操作则是基于图的拓扑结构，直接在空间上定义图卷积操作。这样便使得图卷积操作可以被运用到结构更复杂的图数据中。它的基本思想是聚合相邻结点的信息并对自己的特征进行更新，可以具有更加灵活多变的图卷积运算的表达式。下文介绍几种常见的基于空域的图卷积模型。</p>
<h2 id="模型举例">模型举例</h2>
<h3 id="nn4g">NN4G</h3>
<p>Neural Network for Graphs（NN4G）是第一个基于空域的图卷积网络。其中图卷积的操作被定义为： <span class="math display">\[
\boldsymbol{h}_{v}^{(k)}=f \left(\boldsymbol{W}^{(k)}\boldsymbol{x}_{v}+\sum_{i=1}^{k-1}\sum_{u\in N(v)}\boldsymbol{\Theta}^{(k)}\boldsymbol{h}_{u}^{(k-1)} \right)
\]</span> 其中<span class="math inline">\(f\)</span>为激活函数，<span class="math inline">\(\boldsymbol{\Theta}\)</span>和<span class="math inline">\(\boldsymbol{W}\)</span>代表图卷积的可学习参数。从中可以看出，邻域结点的信息是通过直接累加的方式被聚合起来。为了记忆每一层的信息，网络中还加入了残差连接和跳跃连接操作。</p>
<p>上述操作还可以写成矩阵的形式： <span class="math display">\[
\boldsymbol{H}^{(k)}=f \left(\boldsymbol{W}^{(k)}\boldsymbol{X}+\sum_{i=1}^{k-1}\boldsymbol{A}\boldsymbol{\Theta}^{(k)}\boldsymbol{H}^{(k-1)} \right)
\]</span> 这一表达式的形式与GCN类似，但是它使用了未正则化的邻接矩阵，因此可能会导致数值不稳定的问题。</p>
<h3 id="dcnn">DCNN</h3>
<p>Diffusion Convolutional Neural Network（DCNN）将图卷积看作是一个扩散过程。它假设信息按照一定的概率在一个结点和它的相邻结点之间传递。在若干轮之后，信息的分布会达到一个平衡状态。它的图卷积计算公式定义如下： <span class="math display">\[
\boldsymbol{H}^{(k)}=f(\boldsymbol{W}^{(k)}\odot \boldsymbol{P}^k \boldsymbol{X})
\]</span> 其中，<span class="math inline">\(\boldsymbol{P}=\boldsymbol{D}^{-1}\boldsymbol{A}\)</span>​代表概率转移矩阵。</p>
<p>DCNN将所有的<span class="math inline">\(\boldsymbol{H}^{(1)},\dots,\boldsymbol{H}^{(k)}\)</span>拼接起来作为最后的输出。而另一种与之相近的Diffusion Graph Convolution（DGC）模型则是将每一步卷积操作得到的结果相加作为最后的输出，即<span class="math inline">\(\boldsymbol{H}=\sum_{k=0}^K \boldsymbol{H}^{(k)}\)</span>。</p>
<h3 id="neural-fps">Neural FPs</h3>
<p>Neural FPs的思想是为每一个不同的度值都学习一个权重矩阵，它的计算公式如下： <span class="math display">\[
\boldsymbol{h}_v^{(t+1)}=f\left(\boldsymbol{W}_{|N(v)|}^{(t+1)}\left(\boldsymbol{h}_{v}^{(t)}+\sum_{u\in N(v)} \boldsymbol{h}_{u}^{(t)}\right)\right)
\]</span> 在规模较小的图上面，它的效果较好。但是对于大规模的图，由于结点的度可能会随之变得很高，这种方法就变得无法适用。</p>
<h3 id="graphsage">GraphSAGE</h3>
<p>Graph SAGE（Sample and AggreGatE）将卷积操作分为采样和聚合两步，计算过程为： <span class="math display">\[
\boldsymbol{h}_{N(v)}^{(t+1)}=\text{AGG}_{t+1}(\{\boldsymbol{h}_u^t,\forall u\in N(v)\}) \\
\boldsymbol{h}_v^{(t+1)}=f\left(\boldsymbol{W}^{(t+1)}\cdot [\boldsymbol{h}_v^{(t)}||\boldsymbol{h}_{N(v)}^{(t+1)}]\right)
\]</span> 在采样过程中，GraphSAGE并未使用所有的相邻结点，而是在每一个结点的相邻结点集合中，随机采样出<span class="math inline">\(k\)</span>个样本组合成一个子集。如果集合内的元素数量大于<span class="math inline">\(k\)</span>则采用有放回采样，反之则使用无放回采样。<span class="math inline">\(\text{AGG}_{t+1}\)</span>​​​代表聚合函数，​可以使用Mean、Pooling和LSTM三种类型。</p>
<h3 id="gin">GIN</h3>
<p>GIN（Graph Isomorphism Network）是为了解决图重构思路而提出的，它的目的是想让图卷积运算产生的图嵌入向量可以识别出图结构是否相同。它的图卷积公式为： <span class="math display">\[
\boldsymbol{h}_v^{(t+1)}=\text{MLP}\left((1+\epsilon^{(t+1)})\boldsymbol{h}_v^{(t)}+\sum_{u\in N(v)}\boldsymbol{h}_u^{(t)}\right)
\]</span> 其中，<span class="math inline">\(\text{MLP}\)</span>代表一个多层感知机的神经网络。</p>
<h3 id="gat">GAT</h3>
<p>Graph Attention Network（GAT）在聚合的过程中使用了Attention机制，它的原理如下： <span class="math display">\[
\boldsymbol{h}_v^{(t+1)}=f\left(\sum_{u\in N(v)}\alpha_{uv}^{(t+1)}\boldsymbol{W}^{(t+1)}\boldsymbol{h}_u^{(t)} \right) \\
\alpha_{uv}^{(t+1)}=\frac{\exp\left(\text{LeakyReLU}\left(\boldsymbol{a}^T [\boldsymbol{W}^{(t+1)}\boldsymbol{h}_v^{(t)}||\boldsymbol{W}^{(t+1)}\boldsymbol{h}_u^{(t)}]\right)\right)}{\sum_{k\in N(v)}\exp\left(\text{LeakyReLU}\left(\boldsymbol{a}^T [\boldsymbol{W}^{(t+1)}\boldsymbol{h}_v^{(t)}||\boldsymbol{W}^{(t+1)}\boldsymbol{h}_k^{(t)}]\right)\right)}
\]</span> 其中，<span class="math inline">\(\boldsymbol{a}^T\)</span>是可学习的参数。​</p>
<p>GAT与GCN的计算过程有些类似，但是它们的区别在于GCN中两个相邻结点之间的权重是通过图自身的结构性质计算出来的定值，而GAT则是通过Attention机制学得这样的权重。它们的区别如下图所示：</p>
<figure>
<img data-src="https://raw.githubusercontent.com/lyf35/blog_figures/main/img/20210801215229.png" alt="image-20210725190709339" /><figcaption aria-hidden="true">image-20210725190709339</figcaption>
</figure>
<p>如果在GAT中使用类似于Transformer的多头注意力机制，并且输入为一个全连接图，那么这样的图卷积操作与Transformer的基本组成模块其实是等价的。因为在Transformer计算注意力值的时候，要为所有输入计算每一个输入值和其它输入之间的注意力值。</p>
<h1 id="通用框架">通用框架</h1>
<p>由于图卷积计算具有许多不同的变体，一些学者根据图卷积的计算原理，抽象出了几种不同的通用框架，例如MoNet、MPNN、NLNN等。下面对MPNN和GN这两种框架进行介绍。</p>
<h2 id="mpnn">MPNN</h2>
<p><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE3MDQuMDEyMTIucGRm">Neural Message Passing for Quantum Chemistry<i class="fa fa-external-link-alt"></i></span>是来自于2017年ICML会议中的一篇文章。文中将一些图卷积运算总结为Message Passing Neural Network（MPNN）框架，并基于这一框架提出了enn-s2s的图神经网络结构，在分子性质的预测上取得了State-of-the-art的表现。</p>
<p>作者提出的MPNN框架可以被用于无向图中，假设结点<span class="math inline">\(v\)</span>的特征被表示为<span class="math inline">\(\boldsymbol{x}_v\)</span>，边的特征被表示为<span class="math inline">\(\boldsymbol{e}_{vw}\)</span>​​，那么信息传递过程可以被表示为如下的表达式： <span class="math display">\[
\boldsymbol{m}_v^{t+1}=\sum_{w\in N(v)} M_t(\boldsymbol{h}_v^t,\boldsymbol{h}_w^t,\boldsymbol{e}_{vw}) \\
\boldsymbol{h}_v^{t+1}=U_t(\boldsymbol{h}_v^t,\boldsymbol{m}_v^{t+1})
\]</span> 其中，上标<span class="math inline">\(t\)</span>​​​​代表时间步，模型中共需要进行<span class="math inline">\(T\)</span>步这样的操作；<span class="math inline">\(N(v)\)</span>代表结点<span class="math inline">\(v\)</span>的相邻结点集合；<span class="math inline">\(\boldsymbol{m}_v^{t+1}\)</span>代表结点<span class="math inline">\(v\)</span>在<span class="math inline">\(t+1\)</span>时刻的消息；<span class="math inline">\(\boldsymbol{h}_v^t\)</span>​​​​代表结点<span class="math inline">\(v\)</span>​​​​在<span class="math inline">\(t\)</span>​​​​时刻的隐藏特征，<span class="math inline">\(\boldsymbol{h}_v^0=\boldsymbol{x}_v\)</span>。<span class="math inline">\(M_t\)</span>和<span class="math inline">\(U_t\)</span>分别代表<span class="math inline">\(t\)</span>​​​​​时刻使用的消息函数和更新函数。</p>
<p>如果要得到图结构自身的特征，则需要使用Readout函数<span class="math inline">\(R\)</span>： <span class="math display">\[
\hat{\boldsymbol{y}}=R(\{\boldsymbol{h}_v^T|v\in G \})
\]</span> 为了满足图的同构性，Readout函数的输出必须保证与结点的顺序无关。</p>
<p>在文章中，作者举了Convolutional Networks for Learning Molecular Fingerprints，Gated Graph Neural Networks，Interaction Networks，Molecular Graph Convolutions，Deep Tensor Neural Networks，以及基于拉普拉斯矩阵的几种图卷积网络，说明它们都可以被归类到MPNN这一框架下。</p>
<p>对于SCNN网络而言，它的计算公式为： <span class="math display">\[
\boldsymbol{y}_j=\sigma\left( \sum_{i=1}^{d_1}\boldsymbol{V}\boldsymbol{F}_{i,j}\boldsymbol{V}^T\boldsymbol{x}_i \right) ~~(j=1,\dots,d_2)
\]</span> 其中，<span class="math inline">\(\boldsymbol{x}_i\)</span>​​为<span class="math inline">\(N\)</span>​​维向量，<span class="math inline">\(i\)</span>​​有<span class="math inline">\(d_1\)</span>​​个取值；<span class="math inline">\(\boldsymbol{y}_j\)</span>​为<span class="math inline">\(N\)</span>​维向量，<span class="math inline">\(j\)</span>​有<span class="math inline">\(d_2\)</span>​​个取值；<span class="math inline">\(\boldsymbol{F}_{i,j}\)</span>​为<span class="math inline">\(N\times N\)</span>​的对角矩阵，包含了可学习参数；<span class="math inline">\(\boldsymbol{V}\)</span>​代表拉普拉斯矩阵的特征向量组成的矩阵。</p>
<p>为了将上述表达式写为按矩阵中的元素表达的形式，我们将<span class="math inline">\(x_{w,i}\)</span>记为结点<span class="math inline">\(w\)</span>的第<span class="math inline">\(i\)</span>个维度，<span class="math inline">\(y_{v,j}\)</span>记为结点<span class="math inline">\(v\)</span>的第<span class="math inline">\(j\)</span>个维度，<span class="math inline">\(x_w\)</span>表示结点<span class="math inline">\(w\)</span>的特征向量，<span class="math inline">\(y_v\)</span>表示结点<span class="math inline">\(v\)</span>的特征向量。<span class="math inline">\(\tilde{L}\)</span>代表维度为<span class="math inline">\(N\times N\times d_1 \times d_2\)</span>的四维tensor，其中的元素为<span class="math inline">\(\tilde{L}_{v,w,i,j}=(\boldsymbol{V}\boldsymbol{F}_{i,j}\boldsymbol{V}^T)_{v,w}\)</span>，<span class="math inline">\(\tilde{L}_{i,j}\)</span>代表<span class="math inline">\(N\times N\)</span>的矩阵，<span class="math inline">\(\tilde{L}_{v,w}\)</span>代表<span class="math inline">\(d_1\times d_2\)</span>​的矩阵。基于上述的记号，SCNN的矩阵运算等价于如下形式： <span class="math display">\[
y_j=\sigma\left(\sum_{i=1}^{d_1}\tilde{L}_{i,j}x_i \right) \\
y_{v,j}=\sigma\left(\sum_{i=1,w=1}^{d_1,N}\tilde{L}_{v,w,i,j}x_{w,i} \right) \\
y_v=\sigma\left(\sum_{w=1}^{N}\tilde{L}_{v,w}x_w \right)
\]</span> 将<span class="math inline">\(y_v\)</span>​记作<span class="math inline">\(h_v^{t+1}\)</span>​，<span class="math inline">\(x_w\)</span>​记作<span class="math inline">\(h_w^t\)</span>​，则上面的公式可改写为MPNN的格式：<span class="math inline">\(M(h_v^t,h_w^t)=\tilde{L}_{v,w} h_w^t\)</span>​，<span class="math inline">\(U(h_v^t,m_v^{t+1})=\sigma(m_v^{t+1})\)</span>​。</p>
<p>对于Kipf和Welling提出的GCN网络，它的图卷积公式为： <span class="math display">\[
H^{l+1}=\sigma\left(\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2} H^l W^l \right)
\]</span> 令<span class="math inline">\(L=\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}\)</span>，则对于结点<span class="math inline">\(v\)</span>而言，它的结点更新函数为： <span class="math display">\[
\begin{aligned}
H_{(v)}^{l+1}=&amp;\sigma(L_{(v)} H^l W^l) \\
=&amp;\sigma\left(\sum_{w}L_{vw}H^l_{(w)} W^l \right)
\end{aligned}
\]</span> 其中，下标<span class="math inline">\((v)\)</span>代表矩阵的第<span class="math inline">\(v\)</span>列。如果我们将上述公式中的行矩阵<span class="math inline">\(H_{(v)}^{l+1}\)</span>改为列矩阵<span class="math inline">\(h_{v}^{t+1}\)</span>，则公式等价于： <span class="math display">\[
h_{v}^{t+1}=\sigma\left((W^l)^T \sum_{w}L_{vw}h_w^t \right)
\]</span> 这就等同于如下的消息函数和更新函数： <span class="math display">\[
M_t(h_v^t,h_w^t)=L_{vw}h_w^t=\tilde{A}_{vw}(deg(v)deg(w))^{-1/2}h_w^t \\
U_t(h_v^t,m_v^{t+1})=\sigma((W^t)^T m^{t+1})
\]</span></p>
<p>由于MPNN框架是在空域图卷积的基础上定义的，因此上述推导也相当于是将一些频域图卷积与空域图卷积定义在了一个统一的框架下。</p>
<h2 id="gn">GN</h2>
Graph Network（GN）框架是一个更加通用的计算框架。GN框架的核心计算单元被称为GN模块（GN block），它包含了三个聚合和更新函数： $$
<span class="math display">\[\begin{aligned}
&amp;\boldsymbol{e}_{k}^{(t+1)}=\phi^e\left(\boldsymbol{e}_{k}^{(t)},\boldsymbol{h}_{r_k}^{(t)},\boldsymbol{h}_{s_k}^{(t)},\boldsymbol{u}^{(t)} \right),~ \boldsymbol{\bar{e}}_{v}^{(t+1)}=\rho^{e\rightarrow h}\left(\boldsymbol{E}_{v}^{(t+1)}\right) \\

&amp;\boldsymbol{h}_{v}^{(t+1)}=\phi^h\left(\boldsymbol{\bar{e}}_{v}^{(t+1)}, \boldsymbol{h}_{v}^{(t)},\boldsymbol{u}^{(t)} \right),~\boldsymbol{\bar{e}}^{(t+1)}=\rho^{e\rightarrow u}\left(\boldsymbol{E}^{(t+1)}\right) \\

&amp;\boldsymbol{u}^{(t+1)}=\phi^u\left(\boldsymbol{\bar{e}}_{v}^{(t+1)}, \boldsymbol{\bar{h}}^{(t+1)},\boldsymbol{u}^{(t)} \right),~ \boldsymbol{\bar{h}}^{(t+1)}=\rho^{h\rightarrow u}\left(\boldsymbol{H}^{(t+1)}\right) \\
\end{aligned}\]</span>
<p>$$ 其中<span class="math inline">\(\boldsymbol{e},\boldsymbol{h},\boldsymbol{u}\)</span>分别对应于边，结点和整个图的特征向量；<span class="math inline">\(r_k\)</span>代表入结点，即接收消息的结点；<span class="math inline">\(s_k\)</span>代表出结点，即发送消息的结点；<span class="math inline">\(\boldsymbol{E}_{v}^{(t+1)}\)</span>代表入结点<span class="math inline">\(v\)</span>接收到的所有消息堆叠起来所构成的矩阵；<span class="math inline">\(\boldsymbol{E}^{(t+1)}\)</span>和<span class="math inline">\(\boldsymbol{H}^{(t+1)}\)</span>​​分别代表所有边和所有结点的特征向量​堆叠起来形成的矩阵。</p>
<p><span class="math inline">\(\phi\)</span>和<span class="math inline">\(\rho\)</span>分别对应于更新函数和聚合函数，它们可以具有不同的表达式。此外，<span class="math inline">\(\rho\)</span>​​函数要求输出与变量输入的顺序无关。</p>
<h1 id="代码示例">代码示例</h1>
<p>下面为使用PyTorch-Geometric函数库搭建SchNet网络结构的代码，它可以用于预测物质的物理和化学性质。这一网络中使用到的图卷积操作属于空域卷积。详细的网络结构可以参考文章<span class="exturl" data-url="aHR0cHM6Ly9wYXBlcnMubmlwcy5jYy9wYXBlci8yMDE3L2ZpbGUvMzAzZWQ0YzY5ODQ2YWIzNmMyOTA0ZDNiYTg1NzMwNTAtUGFwZXIucGRm">SchNet: A continuous-filter convolutional neural network for modeling quantum interactions<i class="fa fa-external-link-alt"></i></span>)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch_geometric</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch_geometric.data <span class="keyword">import</span> Data,DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> MessagePassing,global_mean_pool,global_add_pool</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RBFLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,cutoff=<span class="number">6</span>,gamma=<span class="number">0.1</span>,rbfkernel_number=<span class="number">300</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(RBFLayer,self).__init__()</span><br><span class="line">        self.cutoff=cutoff</span><br><span class="line">        self.gamma=gamma</span><br><span class="line">        self.rbfkernel_number=rbfkernel_number</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,g</span>):</span><br><span class="line">        dist=torch.index_select(g.pos,<span class="number">0</span>,g.edge_index[<span class="number">1</span>])-torch.index_select(g.pos,<span class="number">0</span>,g.edge_index[<span class="number">0</span>])</span><br><span class="line">        dist=torch.add(dist,g.edge_attr)</span><br><span class="line">        dist=torch.norm(dist,p=<span class="number">2</span>,dim=<span class="number">1</span>,keepdim=<span class="literal">True</span>)</span><br><span class="line">        centers=np.linspace(<span class="number">0</span>,self.cutoff,self.rbfkernel_number,dtype=np.float32)</span><br><span class="line">        rbf_kernel=torch.tensor(centers,device=<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">        rbf_tensor=dist-rbf_kernel</span><br><span class="line">        rbf_tensor=torch.exp(-self.gamma*torch.mul(rbf_tensor,rbf_tensor))</span><br><span class="line">        <span class="keyword">return</span> rbf_tensor,dist</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FilterGeneratorBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,rbfkernel_number=<span class="number">300</span>,out_dim=<span class="number">64</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(FilterGeneratorBlock,self).__init__()</span><br><span class="line">        self.linear1=nn.Linear(rbfkernel_number,out_dim)</span><br><span class="line">        self.linear2=nn.Linear(out_dim,out_dim)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,rbf_tensor</span>):</span><br><span class="line">        weight=self.linear1(rbf_tensor)</span><br><span class="line">        weight=torch.log(torch.exp(weight)+<span class="number">1.0</span>)-torch.log(torch.tensor(<span class="number">2.0</span>))</span><br><span class="line">        weight=self.linear2(weight)</span><br><span class="line">        weight=torch.log(torch.exp(weight)+<span class="number">1.0</span>)-torch.log(torch.tensor(<span class="number">2.0</span>))</span><br><span class="line">        <span class="keyword">return</span> weight</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">cfconv</span>(<span class="title class_ inherited__">MessagePassing</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,rbfkernel_number=<span class="number">300</span>,feat_num=<span class="number">64</span>,aggragate=<span class="string">&#x27;add&#x27;</span>,node_flow=<span class="string">&#x27;target_to_source&#x27;</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(cfconv,self).__init__(aggr=aggragate,flow=node_flow)</span><br><span class="line">        self.filter_block=FilterGeneratorBlock(rbfkernel_number=rbfkernel_number,out_dim=feat_num)</span><br><span class="line">     </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">message</span>(<span class="params">self,x_j,weight</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.mul(x_j,weight)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self,aggr_out</span>):</span><br><span class="line">        <span class="keyword">return</span> aggr_out</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x,edge_index,rbf_tensor,dist,cutoff</span>):</span><br><span class="line">        weight=self.filter_block(rbf_tensor)</span><br><span class="line">        weight=weight*(<span class="number">1</span>+torch.cos(<span class="number">3.14159265</span>*dist/cutoff))</span><br><span class="line">        <span class="keyword">return</span> self.propagate(edge_index,x=x,weight=weight)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InteractionBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,rbfkernel_number=<span class="number">300</span>,feat_num=<span class="number">64</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(InteractionBlock,self).__init__()</span><br><span class="line">        self.linear1=nn.Linear(feat_num,feat_num)</span><br><span class="line">        self.cfconvlayer=cfconv(rbfkernel_number=rbfkernel_number,feat_num=feat_num)</span><br><span class="line">        self.linear2=nn.Linear(feat_num,feat_num)</span><br><span class="line">        self.linear3=nn.Linear(feat_num,feat_num)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,edge_index,node_feature,rbf_tensor,dist,cutoff</span>):</span><br><span class="line">        x=torch.tensor(node_feature)</span><br><span class="line">        node_feature=self.linear1(node_feature)</span><br><span class="line">        node_feature=self.cfconvlayer(node_feature,edge_index,rbf_tensor,dist,cutoff)</span><br><span class="line">        node_feature=self.linear2(node_feature)</span><br><span class="line">        node_feature=torch.log(torch.exp(node_feature)+<span class="number">1.0</span>)-torch.log(torch.tensor(<span class="number">2.0</span>))</span><br><span class="line">        node_feature=self.linear3(node_feature)</span><br><span class="line">        node_feature=torch.add(node_feature,x)</span><br><span class="line">        <span class="keyword">return</span> node_feature</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Schnet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,cutoff=<span class="number">6</span>,gamma=<span class="number">0.5</span>,rbfkernel_number=<span class="number">300</span>,hidden_layer_dimensions=<span class="number">64</span>,num_conv=<span class="number">3</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Schnet,self).__init__()</span><br><span class="line">        self.num_conv=num_conv</span><br><span class="line">        self.cutoff=cutoff</span><br><span class="line">        </span><br><span class="line">        self.rbf_layer=RBFLayer(cutoff=cutoff,gamma=gamma,rbfkernel_number=rbfkernel_number)</span><br><span class="line">        self.embedding=nn.Embedding(<span class="number">3</span>,hidden_layer_dimensions)</span><br><span class="line">        self.interaction_blocks=nn.ModuleList([InteractionBlock(feat_num=hidden_layer_dimensions,</span><br><span class="line">                                                                rbfkernel_number=rbfkernel_number) </span><br><span class="line">                                               <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_conv)])</span><br><span class="line">        self.atomwise1=nn.Linear(hidden_layer_dimensions,<span class="built_in">int</span>(hidden_layer_dimensions/<span class="number">2</span>))</span><br><span class="line">        self.atomwise2=nn.Linear(<span class="built_in">int</span>(hidden_layer_dimensions/<span class="number">2</span>),<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,g</span>):</span><br><span class="line">        rbf_tensor,dist=self.rbf_layer(g)</span><br><span class="line">        temp=self.embedding(g.x)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_conv):</span><br><span class="line">            temp=self.interaction_blocks[i](g.edge_index,temp,rbf_tensor,dist,self.cutoff)</span><br><span class="line">        temp=self.atomwise1(temp)</span><br><span class="line">        temp=torch.log(torch.exp(temp)+<span class="number">1.0</span>)-torch.log(torch.tensor(<span class="number">2.0</span>))</span><br><span class="line">        temp=self.atomwise2(temp)</span><br><span class="line">        temp=global_add_pool(temp,g.batch)</span><br><span class="line">        <span class="keyword">return</span> temp</span><br></pre></td></tr></table></figure>
<h1 id="参考">参考</h1>
<ol type="1">
<li>Wu, Z. et al. A Comprehensive Survey on Graph Neural Networks.</li>
<li>Zhou, J. et al. Graph neural networks: A review of methods and applications.</li>
<li>Zhang, Z., Cui, P. &amp; Zhu, W. Deep Learning on Graphs: A Survey.</li>
<li><span class="exturl" data-url="aHR0cHM6Ly93d3cuemhpaHUuY29tL3F1ZXN0aW9uLzU0NTA0NDcx">如何理解 Graph Convolutional Network（GCN）？ - 知乎 (zhihu.com)<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly90a2lwZi5naXRodWIuaW8vZ3JhcGgtY29udm9sdXRpb25hbC1uZXR3b3Jrcy8=">Graph Convolutional Networks | Thomas Kipf | University of Amsterdam (tkipf.github.io)<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTkwMTUxOS9hcnRpY2xlL2RldGFpbHMvMTA2Mzg4OTY0">图卷积神经网络笔记——第二章：谱域图卷积介绍（1）_Ma Sizhou-CSDN博客_谱域图卷积<i class="fa fa-external-link-alt"></i></span>，<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTkwMTUxOS9hcnRpY2xlL2RldGFpbHMvMTA2NDM2NTkx">图卷积神经网络笔记——第二章：谱域图卷积介绍（2）_Ma Sizhou-CSDN博客<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zNjI0MTYxMjQ=">理解图的拉普拉斯矩阵 - 知乎 (zhihu.com)<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzEzMTIuNjIwMy5wZGY=">Spectral Networks and Deep Locally Connected Networks on Graphs<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9wYXBlcnMubmlwcy5jYy9wYXBlci8yMDE2L2ZpbGUvMDRkZjRkNDM0ZDQ4MWM1YmI3MjNiZTFiNmRmMWVlNjUtUGFwZXIucGRm">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering (nips.cc)<i class="fa fa-external-link-alt"></i></span></li>
<li><span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzE2MDkuMDI5MDd2NC5wZGY=">Semi-Supervised Classification with Graph Convolutional Networks<i class="fa fa-external-link-alt"></i></span></li>
</ol>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Yufei Luo
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://lyf35.github.io/2021/07/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" title="深度学习-图卷积网络">http://lyf35.github.io/2021/07/20/深度学习-图卷积网络/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</span> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 深度学习</a>
              <a href="/tags/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag"><i class="fa fa-tag"></i> 图神经网络</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/07/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A6%82%E8%BF%B0/" rel="prev" title="深度学习-图神经网络概述">
      <i class="fa fa-chevron-left"></i> 深度学习-图神经网络概述
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/07/23/%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86-%E5%82%85%E9%87%8C%E5%8F%B6%E5%88%86%E6%9E%90/" rel="next" title="信号处理-傅里叶分析">
      信号处理-傅里叶分析 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



        </div>
        
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2020 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yufei Luo</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">1.5m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">22:56</span>
</div>
  <div class="powered-by">由 <span class="exturl theme-link" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl theme-link" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZy9waXNjZXMv">NexT.Pisces</span> 强力驱动
  </div>

        






<script data-pjax>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      const visitors = document.querySelector('.leancloud_visitors');
      const url = decodeURI(visitors.id);
      const title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            const counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              Counter('post', '/classes/Counter', { title, url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      const visitors = document.querySelectorAll('.leancloud_visitors');
      const entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"ypNNXSdcGIIoldC5BBmViS2g-gzGzoHsz","app_key":"0TDwohU7eO4yHTn4tley8PtE","server_url":"https://leancloud.cn/dashboard/data.html?appid=ypNNXSdcGIIoldC5BBmViS2g-gzGzoHsz#/","security":false};
    function fetchData(api_server) {
      const Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    const api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.6" zIndex="-1" src="//cdn.jsdelivr.net/npm/ribbon.js@1/dist/ribbon.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/next-theme/pjax@0/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '.page-configurations',
    '.main-inner',
    '.post-toc-wrap',
    '.languages',
    '.pjax'
  ],
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

document.addEventListener('pjax:success', () => {
  pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  const hasTOC = document.querySelector('.post-toc');
  document.querySelector('.sidebar-inner').classList.toggle('sidebar-nav-active', hasTOC);
  document.querySelector(hasTOC ? '.sidebar-nav-toc' : '.sidebar-nav-overview').click();
  NexT.utils.updateSidebarPosition();
});
</script>


  
  <script data-pjax>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  
<script src="/js/local-search.js"></script>









<script data-pjax>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>


<script data-pjax>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.init({
      theme    : 'forest',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    }, '.mermaid');
  }, window.mermaid);
}
</script>


    <div class="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      const script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments('#gitalk-container', () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '4dcdc02d24075b0e9d3a',
      clientSecret: '53cf2cb8b89b9d4bba922bb19bda1290f0d0bf95',
      repo        : 'lyf35.github.io',
      owner       : 'lyf35',
      admin       : ['lyf35'],
      id          : 'e3ba4c490b95515c8b00b09d0bcaf3c4',
        language: 'zh-CN',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

    </div>
</body>
</html>
