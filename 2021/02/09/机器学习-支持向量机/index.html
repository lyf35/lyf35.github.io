<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#222">
  <link rel="manifest" href="/images/site.webmanifest">
  <meta name="msapplication-config" content="/images/browserconfig.xml">
  <meta name="msvalidate.01" content="<meta name="msvalidate.01" content="5D3796A5DDB32CC875380613FB613833" />">
  <meta name="baidu-site-verification" content="<meta name="baidu-site-verification" content="kewHZtFYQk" />">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Helvetica:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  
  <link rel="stylesheet" href="/lib/animate-css/animate.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-flat-top.min.css">
  <script src="//cdn.jsdelivr.net/npm/pace-js@1/pace.min.js"></script>

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"lyf35.github.io","root":"/","scheme":"Pisces","version":"8.0.0-rc.5","exturl":true,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":{"gitalk":{"order":-2}},"activeClass":"gitalk"},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"path":"search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>

  <meta name="description" content="定义 支持向量机（Support Vector Machine，SVM）是一种二分类模型，属于线性模型的一种，它的学习思想是在空间内找到一个超平面\(\boldsymbol{x}\boldsymbol{w}^T+b&#x3D;0\)（其中\(\boldsymbol{x}&#x3D;(x_1,x_2,\dots,x_n)\)代表模型的输入向量，\(\boldsymbol{w}&#x3D;(w_1,w_2,\dots,w_n)\)">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习-支持向量机">
<meta property="og:url" content="http://lyf35.github.io/2021/02/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/index.html">
<meta property="og:site_name" content="Yufei Luo&#39;s Blog">
<meta property="og:description" content="定义 支持向量机（Support Vector Machine，SVM）是一种二分类模型，属于线性模型的一种，它的学习思想是在空间内找到一个超平面\(\boldsymbol{x}\boldsymbol{w}^T+b&#x3D;0\)（其中\(\boldsymbol{x}&#x3D;(x_1,x_2,\dots,x_n)\)代表模型的输入向量，\(\boldsymbol{w}&#x3D;(w_1,w_2,\dots,w_n)\)">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/lyf35/blog_figures/main/img/20210301212934.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/lyf35/blog_figures/main/img/20210301213008">
<meta property="og:image" content="https://raw.githubusercontent.com/lyf35/blog_figures/main/img/20210301213455.png">
<meta property="og:image" content="https://raw.githubusercontent.com/lyf35/blog_figures/main/img/20210301213535">
<meta property="og:image" content="https://raw.githubusercontent.com/lyf35/blog_figures/main/img/20210402080210.png">
<meta property="article:published_time" content="2021-02-09T10:52:34.000Z">
<meta property="article:modified_time" content="2021-04-02T00:45:14.000Z">
<meta property="article:author" content="Yufei Luo">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="监督学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/lyf35/blog_figures/main/img/20210301212934.jpg">

<link rel="canonical" href="http://lyf35.github.io/2021/02/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/">


<script data-pjax class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>机器学习-支持向量机 | Yufei Luo's Blog</title>
  






  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
 <!--   <div class="headband"></div>-->

    <main class="main">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader">
        <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Yufei Luo's Blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">But I was so much older then, I am younger than that now.</p>
      <img class="custom-logo-image" src="/images/logo.png" alt="Yufei Luo's Blog">
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-计算机基础">

    <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80" rel="section"><i class="fa fa-tags fa-fw"></i>计算机基础</a>

  </li>
        <li class="menu-item menu-item-机器学习">

    <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0" rel="section"><i class="fa fa-tags fa-fw"></i>机器学习</a>

  </li>
        <li class="menu-item menu-item-深度学习">

    <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0" rel="section"><i class="fa fa-tags fa-fw"></i>深度学习</a>

  </li>
        <li class="menu-item menu-item-工程实践">

    <a href="/categories/%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5" rel="section"><i class="fa fa-tags fa-fw"></i>工程实践</a>

  </li>
        <li class="menu-item menu-item-论文笔记">

    <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0" rel="section"><i class="fa fa-tags fa-fw"></i>论文笔记</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404" rel="section"><i class="fa fa-heartbeat fa-fw"></i>公益 404</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <section class="post-toc-wrap sidebar-panel">
          <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89"><span class="nav-number">1.</span> <span class="nav-text">定义</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%8E%A8%E5%AF%BC"><span class="nav-number">2.</span> <span class="nav-text">模型推导</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A1%AC%E9%97%B4%E9%9A%94svm"><span class="nav-number">2.1.</span> <span class="nav-text">硬间隔SVM</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E5%AF%BC%E5%87%BA"><span class="nav-number">2.1.1.</span> <span class="nav-text">优化问题的导出</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B1%82%E8%A7%A3%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98"><span class="nav-number">2.1.2.</span> <span class="nav-text">求解优化问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BD%AF%E9%97%B4%E9%9A%94svm"><span class="nav-number">2.2.</span> <span class="nav-text">软间隔SVM</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E4%BF%AE%E6%94%B9"><span class="nav-number">2.2.1.</span> <span class="nav-text">优化问题的修改</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B1%82%E8%A7%A3%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98-1"><span class="nav-number">2.2.2.</span> <span class="nav-text">求解优化问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%8E%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%A7%92%E5%BA%A6%E7%9C%8Bsvm"><span class="nav-number">2.3.</span> <span class="nav-text">从损失函数角度看SVM</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E5%9B%9E%E5%BD%92"><span class="nav-number">3.</span> <span class="nav-text">支持向量机回归</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E5%AF%BC%E5%87%BA"><span class="nav-number">3.1.</span> <span class="nav-text">问题导出</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B1%82%E8%A7%A3%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98-2"><span class="nav-number">3.2.</span> <span class="nav-text">求解优化问题</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%B1%82%E8%A7%A3smo%E7%AE%97%E6%B3%95"><span class="nav-number">4.</span> <span class="nav-text">模型求解—SMO算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%AD%E4%BB%A3%E5%85%AC%E5%BC%8F%E7%9A%84%E5%AF%BC%E5%87%BA"><span class="nav-number">4.1.</span> <span class="nav-text">迭代公式的导出</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%AD%E4%BB%A3%E7%BB%93%E6%9E%9C%E7%9A%84%E4%BF%AE%E5%89%AA"><span class="nav-number">4.2.</span> <span class="nav-text">迭代结果的修剪</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%B4%E7%95%8C%E6%83%85%E5%86%B5%E7%9A%84%E5%A4%84%E7%90%86"><span class="nav-number">4.3.</span> <span class="nav-text">临界情况的处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%81%8F%E7%BD%AE%E7%9A%84%E6%9B%B4%E6%96%B0"><span class="nav-number">4.4.</span> <span class="nav-text">偏置的更新</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%98%E9%87%8F%E7%9A%84%E9%80%89%E6%8B%A9"><span class="nav-number">4.5.</span> <span class="nav-text">变量的选择</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="nav-number">5.</span> <span class="nav-text">优缺点</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B"><span class="nav-number">6.</span> <span class="nav-text">代码示例</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E7%B1%BB"><span class="nav-number">6.1.</span> <span class="nav-text">分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9E%E5%BD%92"><span class="nav-number">6.2.</span> <span class="nav-text">回归</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%99%84%E5%B9%BF%E4%B9%89%E7%BA%A6%E6%9D%9F%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98"><span class="nav-number">7.</span> <span class="nav-text">附：广义约束优化问题</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E5%AD%90%E6%B3%95%E4%B8%8Ekkt%E6%9D%A1%E4%BB%B6"><span class="nav-number">7.1.</span> <span class="nav-text">拉格朗日乘子法与KKT条件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98"><span class="nav-number">7.2.</span> <span class="nav-text">拉格朗日对偶问题</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">8.</span> <span class="nav-text">参考</span></a></li></ol></div>
      </section>
      <!--/noindex-->

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Yufei Luo"
      src="/images/avatar.jpeg">
  <p class="site-author-name" itemprop="name">Yufei Luo</p>
  <div class="site-description" itemprop="description">哪怕什么真理无穷，进一寸有进一寸的欢喜</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
        
          <span class="site-state-item-count">90</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories">
          
        <span class="site-state-item-count">24</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags">
          
        <span class="site-state-item-count">30</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2x5ZjM1" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;lyf35"><i class="fab fa-github fa-fw"></i>GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOmx5ZjEyMzQwMDAwMDBAMTYzLmNvbQ==" title="E-Mail → mailto:lyf1234000000@163.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</span>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <span class="exturl cc-opacity" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></span>
  </div>



      </section>
        <div class="back-to-top animated">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </header>

      
  <div class="reading-progress-bar"></div>

  <span class="exturl github-corner" data-url="aHR0cHM6Ly9naXRodWIuY29tL2x5ZjM1" title="Follow me on GitHub" aria-label="Follow me on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></span>

<noscript>
  <div id="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


      <div class="main-inner">
        

        <div class="content post posts-expand">
          

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://lyf35.github.io/2021/02/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="Yufei Luo">
      <meta itemprop="description" content="哪怕什么真理无穷，进一寸有进一寸的欢喜">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yufei Luo's Blog">
    </span>

    
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习-支持向量机
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-02-09 18:52:34" itemprop="dateCreated datePublished" datetime="2021-02-09T18:52:34+08:00">2021-02-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-04-02 08:45:14" itemprop="dateModified" datetime="2021-04-02T08:45:14+08:00">2021-04-02</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%90%86%E8%AE%BA%E7%9F%A5%E8%AF%86/" itemprop="url" rel="index"><span itemprop="name">理论知识</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%90%86%E8%AE%BA%E7%9F%A5%E8%AF%86/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          
            <span id="/2021/02/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" class="post-meta-item leancloud_visitors" data-flag-title="机器学习-支持向量机" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>26k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>23 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="定义">定义</h1>
<p><strong>支持向量机</strong>（Support Vector Machine，SVM）是一种二分类模型，属于线性模型的一种，它的学习思想是在空间内找到一个超平面<span class="math inline">\(\boldsymbol{x}\boldsymbol{w}^T+b=0\)</span>（其中<span class="math inline">\(\boldsymbol{x}=(x_1,x_2,\dots,x_n)\)</span>代表模型的输入向量，<span class="math inline">\(\boldsymbol{w}=(w_1,w_2,\dots,w_n)\)</span>与<span class="math inline">\(b\)</span>为模型参数），能够使得两类样本与超平面的间距最大化。落在超平面两侧的样本，分别对应于<span class="math inline">\(y=+1\)</span>和<span class="math inline">\(y=-1\)</span>两种不同的分类。也就是说，在学得超平面的表达式之后，支持向量机模型的分类函数可以写为： <span class="math display">\[
y=
\begin{cases}
1,~\boldsymbol{x}\boldsymbol{w}^T+b&gt;0 \\
-1,~\boldsymbol{x}\boldsymbol{w}^T+b&lt;0 \\
\end{cases}
\]</span> <span id="more"></span></p>
<p><img data-src="https://raw.githubusercontent.com/lyf35/blog_figures/main/img/20210301212934.jpg" alt="img" style="zoom:67%;" /></p>
<p>上图所示为SVM在二维平面内的一个简单示例，图中的黑点和白点代表两种不同的类别。在图中有三条不同的直线<span class="math inline">\(H_1,H_2,H_3\)</span>，我们可以看到，<span class="math inline">\(H_1\)</span>无法将两种类别的样本正确地分类；<span class="math inline">\(H_2\)</span>和<span class="math inline">\(H_3\)</span>都可以将两种类别的样本正确地分类，但是<span class="math inline">\(H_2\)</span>与最近的数据点之间的间隔明显比<span class="math inline">\(H_3\)</span>与最近的数据点之间的间隔要小得多。因此我们有理由推测，<span class="math inline">\(H_3\)</span>在处理未知数据的时候应该对样本点有更好的分类效果。SVM模型便对应于图中<span class="math inline">\(H_3\)</span>这条直线的表达式，而距离<span class="math inline">\(H_3\)</span>最近的那个黑点和白点被称为<strong>支持向量</strong>。</p>
<h1 id="模型推导">模型推导</h1>
<h2 id="硬间隔svm">硬间隔SVM</h2>
<h3 id="优化问题的导出">优化问题的导出</h3>
<p>求解SVM的过程就是求解能够使得两类样本间隔最大的超平面的过程，可以将其转化为优化问题进行求解。简单起见，我们先讨论硬间隔SVM，即超平面与支持向量形成的间隔中不允许有任何样本点；然后再对这一限制条件进行松弛，允许间隔中有部分样本点的出现，从而得到软间隔SVM。</p>
<p>在空间内，任意一个样本<span class="math inline">\(\boldsymbol{x}_i\)</span>与超平面<span class="math inline">\(\boldsymbol{x}\boldsymbol{w}^T+b=0\)</span>之间的距离<span class="math inline">\(d=\frac{||\boldsymbol{x}_i\boldsymbol{w}^T+b||}{||\boldsymbol{w}||}\)</span>。假设在所有样本中，与超平面距离的最小值为<span class="math inline">\(d_{\min}=\min \frac{||\boldsymbol{x}_i\boldsymbol{w}^T+b||}{||\boldsymbol{w}||}\)</span>，则SVM的优化问题可以表述为下面的形式： <span class="math display">\[
\max~d_{\min} \\
s.t.~y_i(\boldsymbol{x}_i\boldsymbol{w}^T+b)\ge d_{min}||\boldsymbol{w}||
\]</span> 由于<span class="math inline">\(\boldsymbol{w}\)</span>和<span class="math inline">\(b\)</span>的取值不做限制，因此表达式<span class="math inline">\(||\boldsymbol{x}_i\boldsymbol{w}^T+b||\)</span>与<span class="math inline">\(||\boldsymbol{w}||\)</span>的值也都是任意的，这增加了求解优化问题的难度。因此，我们需要对上述的优化问题做一些修改，使其变成容易求解的形式。</p>
<p>由于我们在优化样本与超平面之间的距离时，其实只需要考虑的是距离超平面最近的那些样本点；加之<span class="math inline">\(\boldsymbol{w}\)</span>和<span class="math inline">\(b\)</span>的取值没有限制，所以<span class="math inline">\(||\boldsymbol{x}_i\boldsymbol{w}^T+b||\)</span>的取值也可以为任何正实数。因此，我们不妨对<span class="math inline">\(\boldsymbol{w}\)</span>和<span class="math inline">\(b\)</span>的取值做一些限制，使得那些距离超平面最近的样本点（即支持向量），表达式<span class="math inline">\(||\boldsymbol{x}_i\boldsymbol{w}^T+b||=1\)</span>成立。这样也意味着，对于其它距离超平面<span class="math inline">\(\boldsymbol{x}\boldsymbol{w}^T+b=0\)</span>不是最近的样本点来说，必然有<span class="math inline">\(||\boldsymbol{x}_i\boldsymbol{w}^T+b||&gt;1\)</span>。如下图所示：</p>
<p><img data-src="https://raw.githubusercontent.com/lyf35/blog_figures/main/img/20210301213008" alt="img" style="zoom: 50%;" /></p>
<p>通过对<span class="math inline">\(\boldsymbol{w}\)</span>和<span class="math inline">\(b\)</span>的取值进行约束之后，SVM的优化问题变为下面这种比较简单的表示形式： <span class="math display">\[
\max~\frac{1}{||\boldsymbol{w}||} \\
s.t.~~y_i(\boldsymbol{x}_i\boldsymbol{w}^T+b)\ge 1
\]</span> 由于求<span class="math inline">\(\frac{1}{||w||}\)</span>的最大值其实相当于是求<span class="math inline">\(\frac{1}{2}||w||^2\)</span>的最小值（通过这样的转换，在求解优化问题的时候能够使得表达式更加简洁，同时也方便后续的求解过程），因此上述优化问题等价于： <span class="math display">\[
\min~\frac{1}{2}||\boldsymbol{w}||^2 \\
s.t.~~y_i(\boldsymbol{x}_i\boldsymbol{w}^T+b)\ge 1
\]</span></p>
<blockquote>
<p>备注：</p>
<p>在一些介绍SVM的相关文章或者书籍中，<span class="math inline">\(||\boldsymbol{x}_i\boldsymbol{w}^T+b||\)</span>被称作函数距离，<span class="math inline">\(\frac{||\boldsymbol{x}_i\boldsymbol{w}^T+b||}{||\boldsymbol{w}||}\)</span>被称作几何距离。函数距离可以为任意的非负实数，而几何距离则为点到直线的真实距离。</p>
</blockquote>
<h3 id="求解优化问题">求解优化问题</h3>
<p><em>说明：此处涉及到的一些数学概念可以参考本文的附录部分。</em></p>
<p>通过对上述的表达式使用拉格朗日乘子法，我们可以得到： <span class="math display">\[
\mathcal{L}(\boldsymbol{w},b,\alpha_i)=\frac{1}{2}||\boldsymbol{w}||^2-\sum_{i=1}^{n}\alpha_i[y_i(\boldsymbol{x}_i\boldsymbol{w}^T+b)-1] \\
s.t.~ \alpha_i\ge 0
\]</span> 因此，我们的优化目标为： <span class="math display">\[
\min_{\boldsymbol{w},b}\max_{\alpha_i \ge 0}\mathcal{L}(\boldsymbol{w},b,\alpha_i)
\]</span> 由于这一优化问题满足KKT条件与Slater条件，因此可以通过求解如下的对偶问题，来获得原始问题的解： <span class="math display">\[
\max_{\alpha_i \ge 0}\min_{\boldsymbol{w},b}\mathcal{L}(\boldsymbol{w},b,\alpha_i)
\]</span> 对于上述的对偶问题，我们首先求解<span class="math inline">\(\min_{\boldsymbol{w},b}\mathcal{L}(\boldsymbol{w},b,\alpha_i)\)</span>。分别对<span class="math inline">\(\boldsymbol{w}\)</span>和<span class="math inline">\(b\)</span>求偏导，可得： <span class="math display">\[
\frac{\partial \mathcal{L}}{\partial \boldsymbol{w}}=\boldsymbol{w}-\sum_{i=1}^{n}\alpha_iy_i\boldsymbol{x}_i \\
\frac{\partial \mathcal{L}}{\partial b}=-\sum_{i=1}^{n}\alpha_iy_i
\]</span> 令上述两个偏导数的值为0，我们可得： <span class="math display">\[
\boldsymbol{w}=\sum_{i=1}^{n}\alpha_iy_i\boldsymbol{x}_i \\
\sum_{i=1}^{n}\alpha_iy_i=0\\
\]</span> 将上述两式代入到<span class="math inline">\(\mathcal{L}(\boldsymbol{w},b,\alpha_i)\)</span>的表达式中，可得： <span class="math display">\[
\min_{\boldsymbol{w},b}\mathcal{L}(\boldsymbol{w},b,\alpha_i)=\sum_{i=1}^{n}\alpha_i-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i\boldsymbol{x}_j^T
\]</span> 这样，待优化表达式中便只剩下<span class="math inline">\(\alpha_i\)</span>。接下来我们便可以通过优化<span class="math inline">\(\alpha_i\)</span>的取值，来求解<span class="math inline">\(\max_{\alpha_i}\sum_{i=1}^{n}\alpha_i-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i\boldsymbol{x}_j^T\)</span>。这一求解过程可以通过SMO算法来求解，在下文中会对其进行详细说明。最终，在得到一系列的<span class="math inline">\(\alpha_i\)</span>的取值之后，我们便可以通过<span class="math inline">\(\boldsymbol{w}=\sum_{i=1}^{n}\alpha_iy_i\boldsymbol{x}_i\)</span>来计算得到<span class="math inline">\(\boldsymbol{w}\)</span>的值，然后使用支持向量去计算<span class="math inline">\(b\)</span>的值。</p>
<h2 id="软间隔svm">软间隔SVM</h2>
<h3 id="优化问题的修改">优化问题的修改</h3>
<p>在上面的硬间隔SVM中，我们假设原始数据是线性可分的情况。但是在实际中，由于噪声以及离群点的影响，常常会遇到数据线性不可分的情况；也或者是数据虽然线性可分，但是硬间隔SVM得到的超平面与支持向量之间的距离很近（也就是说支持向量与超平面形成的间隔过窄），从而出现过拟合的现象。此时可以对约束条件进行一些松弛，允许部分样本落入到间隔内，甚至是被分类到错误的类别中去。这样得到的SVM模型被称为软间隔SVM。</p>
<p>对于软间隔SVM，它的优化问题如下： <span class="math display">\[
\min~\frac{1}{2}||\boldsymbol{w}||^2+C\sum_{i=1}^{n}\xi_i \\
\begin{aligned}
s.t.~&amp;y_i(\boldsymbol{x}_i\boldsymbol{w}^T+b)\ge 1-\xi_i\\
&amp;\xi_i\ge 0
\end{aligned}
\]</span> 其中<span class="math inline">\(C&gt;0\)</span>为模型的超参数，代表对落入间隔或者被错误分类的样本的惩罚。<span class="math inline">\(C\)</span>的值越大，代表惩罚越大，如果<span class="math inline">\(C=+\infty\)</span>则变成了硬间隔SVM。</p>
<h3 id="求解优化问题-1">求解优化问题</h3>
<p>对于这一优化问题，我们同样使用拉格朗日乘子法进行求解。首先构造如下的拉格朗日函数： <span class="math display">\[
\mathcal{L}(\boldsymbol{w},b,\alpha_i,\beta_i,\xi_i)=\frac{1}{2}||\boldsymbol{w}||^2+C\sum_{i=1}^{n}\xi_i-\sum_{i=1}^{n}\alpha_i[y_i(\boldsymbol{x}_i\boldsymbol{w}^T+b)-1+\xi_i]-\sum_{i=1}^{n}\beta_i\xi_i \\
\begin{aligned}
s.t.~&amp; \alpha_i\ge 0 \\
&amp;\beta_i\ge 0
\end{aligned}
\]</span> 因此，我们的优化目标为： <span class="math display">\[
\min_{\boldsymbol{w},b,\xi_i}\max_{\alpha_i \ge 0,\beta_i \ge 0}\mathcal{L}(\boldsymbol{w},b,\alpha_i,\beta_i,\xi_i)
\]</span> 类似地，这一优化问题满足KKT条件与Slater条件，因此可以通过求解如下的对偶问题，来获得原始问题的解： <span class="math display">\[
\max_{\alpha_i \ge 0,\beta_i \ge 0}\min_{\boldsymbol{w},b,\xi_i}\mathcal{L}(\boldsymbol{w},b,\alpha_i,\beta_i,\xi_i)
\]</span> 对于上述的对偶问题，我们首先求解<span class="math inline">\(\min_{\boldsymbol{w},b}\mathcal{L}(\boldsymbol{w},b,\alpha_i,\beta_i,\xi_i)\)</span>。分别对<span class="math inline">\(\boldsymbol{w}\)</span>，<span class="math inline">\(b\)</span>和<span class="math inline">\(\xi_i\)</span>求偏导，可得： <span class="math display">\[
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial \boldsymbol{w}}&amp;=\boldsymbol{w}-\sum_{i=1}^{n}\alpha_iy_i\boldsymbol{x}_i \\
\frac{\partial \mathcal{L}}{\partial b}&amp;=-\sum_{i=1}^{n}\alpha_iy_i \\
\frac{\partial \mathcal{L}}{\partial \xi_i}&amp;=C-\alpha_i-\beta_i
\end{aligned}
\]</span> 令上述三个偏导数的值为0，我们可得： <span class="math display">\[
\boldsymbol{w}=\sum_{i=1}^{n}\alpha_iy_i\boldsymbol{x}_i \\
\sum_{i=1}^{n}\alpha_iy_i=0\\
\alpha_i+\beta_i=C
\]</span> 将上述三式代入到<span class="math inline">\(\mathcal{L}(\boldsymbol{w},b,\alpha_i,\beta_i,\xi_i)\)</span>的表达式中，可以消去所有关于<span class="math inline">\(\xi_i\)</span>的表达式，但是表达式<span class="math inline">\(\alpha_i+\beta_i=C\)</span>却为<span class="math inline">\(\alpha_i\)</span>加入了新的限制条件。此时可得： <span class="math display">\[
\min_{\boldsymbol{w},b}\mathcal{L}(\boldsymbol{w},b,\alpha_i,\beta_i,\xi_i)=\sum_{i=1}^{n}\alpha_i-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i\boldsymbol{x}_j^T \\
s.t.~0\le \alpha_i \le C
\]</span> 接下来便是通过优化<span class="math inline">\(\alpha_i\)</span>来求解优化问题<span class="math inline">\(\max_{\alpha_i}\sum_{i=1}^{n}\alpha_i-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i\boldsymbol{x}_j^T\)</span>，这一问题的求解与硬间隔SVM一样使用SMO算法，但是在优化过程中需要对<span class="math inline">\(\alpha_i\)</span>的取值进行限制。</p>
<h2 id="从损失函数角度看svm">从损失函数角度看SVM</h2>
<p>SVM的另一个解读是从它的损失函数出发。考虑如下的折页损失函数（Hinge Loss）： <span class="math display">\[
L(\boldsymbol{x},y)=\max[0,1-y(\boldsymbol{x}\boldsymbol{w}^T+b)]
\]</span> 折页损失函数的值与支持向量机模型的分类结果有如下几种对应的情形：</p>
<ol type="1">
<li><span class="math inline">\(y\)</span>与<span class="math inline">\(\boldsymbol{x}\boldsymbol{w}^T+b\)</span>同号，且<span class="math inline">\(||\boldsymbol{x}\boldsymbol{w}^T+b||&gt;1\)</span>。代表样本被正确分类，且样本在分类间隔的两侧。此时<span class="math inline">\(1-y(\boldsymbol{x}\boldsymbol{w}^T+b)&lt;0\)</span>，因此对应的损失函数值为0。</li>
<li><span class="math inline">\(y\)</span>与<span class="math inline">\(\boldsymbol{x}\boldsymbol{w}^T+b\)</span>同号，且<span class="math inline">\(||\boldsymbol{x}\boldsymbol{w}^T+b||=1\)</span>。代表样本被正确分类，且样本刚好位于分类间隔的边界处。此时<span class="math inline">\(1-y(\boldsymbol{x}\boldsymbol{w}^T+b)=0\)</span>，对应于损失函数的临界值。</li>
<li><span class="math inline">\(y\)</span>与<span class="math inline">\(\boldsymbol{x}\boldsymbol{w}^T+b\)</span>同号，但<span class="math inline">\(0\le ||\boldsymbol{x}\boldsymbol{w}^T+b||&lt; 1\)</span>；或<span class="math inline">\(y\)</span>与<span class="math inline">\(\boldsymbol{x}\boldsymbol{w}^T+b\)</span>异号。代表样本点落在分类间隔内，或是被错误分类。此时<span class="math inline">\(1-y(\boldsymbol{x}\boldsymbol{w}^T+b)&gt;0\)</span>，因此损失函数的值也为<span class="math inline">\(1-y(\boldsymbol{x}\boldsymbol{w}^T+b)\)</span>。</li>
</ol>
<p>因此，SVM的损失函数<span class="math inline">\(\ell(\boldsymbol{w},b)\)</span>相当于是带有L2正则化的折页损失函数，即： <span class="math display">\[
\ell(\boldsymbol{w},b)=\sum_{i=0}^{n} \max[0,1-y_i(\boldsymbol{x}_i\boldsymbol{w}^T+b)]+\lambda||\boldsymbol{w}||^2
\]</span> SVM的优化过程就是求<span class="math inline">\(\ell(\boldsymbol{w},b)\)</span>的最小值。</p>
<p>我们令<span class="math inline">\(\max[0,1-y_i(\boldsymbol{x}_i\boldsymbol{w}^T+b)]=\xi_i\)</span>，其中<span class="math inline">\(\xi_i\ge 0\)</span>。因此有：<span class="math inline">\(1-y_i(\boldsymbol{x}_i\boldsymbol{w}^T+b)\le \xi_i\)</span>，也就是<span class="math inline">\(y_i(\boldsymbol{x}_i\boldsymbol{w}^T+b)\ge 1-\xi_i\)</span>。将其代入到<span class="math inline">\(\ell(\boldsymbol{w},b)\)</span>中，可得： <span class="math display">\[
\ell(\boldsymbol{w},b)=\sum_{i=0}^{n}\xi_i+\lambda||\boldsymbol{w}||^2 \\
\begin{aligned}
s.t.~&amp;y_i(\boldsymbol{x}_i\boldsymbol{w}^T+b)\ge 1-\xi_i \\
&amp;\xi_i\ge 0
\end{aligned}
\]</span> 因此，求解<span class="math inline">\(\ell(\boldsymbol{w},b)\)</span>的最小值等价于软间隔SVM的优化问题。这一优化问题的求解可参考软间隔SVM部分，不再赘述。</p>
<h1 id="支持向量机回归">支持向量机回归</h1>
<h2 id="问题导出">问题导出</h2>
<p>支持向量机模型也可以用来解决回归问题。使用SVM做回归问题时，使用如下的<span class="math inline">\(\epsilon\)</span>-sensitive误差函数来计算回归误差： <span class="math display">\[
L(\boldsymbol{x},y)=\max[0,|y-(\boldsymbol{x}\boldsymbol{w}^T+b)|-\epsilon]
\]</span> 其中，<span class="math inline">\(\epsilon&gt;0\)</span>代表模型的超参数。</p>
<p>如果从几何角度考虑这一损失函数，相当于是超平面<span class="math inline">\(y=\boldsymbol{x}\boldsymbol{w}^T+b+\epsilon\)</span>与<span class="math inline">\(y=\boldsymbol{x}\boldsymbol{w}^T+b-\epsilon\)</span>构成了一个宽度为<span class="math inline">\(2\epsilon\)</span>的间隔。当样本点落在这一间隔内时，对应损失函数的值为0；而样本落在这一间隔外时，对应损失函数的值大于0。在对一个未知样本点做预测时，通过<span class="math inline">\(y=\boldsymbol{x}\boldsymbol{w}^T+b\)</span>进行预测，相当于是取间隔的中间位置。如下图所示：</p>
<figure>
<img data-src="https://raw.githubusercontent.com/lyf35/blog_figures/main/img/20210301213455.png" alt="在这里插入图片描述" /><figcaption aria-hidden="true">在这里插入图片描述</figcaption>
</figure>
<p>与支持向量机做分类时类似，支持向量机回归的损失函数<span class="math inline">\(\ell(\boldsymbol{w},b)\)</span>也是带有L2正则化的<span class="math inline">\(\epsilon\)</span>-sensitive损失函数： <span class="math display">\[
\ell(\boldsymbol{w},b)=\sum_{i=1}^{n}\max[0,|y_i-(\boldsymbol{x}_i\boldsymbol{w}^T+b)|-\epsilon]+\lambda||\boldsymbol{w}||^2
\]</span> 求解模型参数即为求解优化问题<span class="math inline">\(\min_{\boldsymbol{w},b}\ell(\boldsymbol{w},b)\)</span>。为此，我们为每个样本点引入两个约束变量<span class="math inline">\(\xi_i\ge 0\)</span>和<span class="math inline">\(\xi_i&#39;\ge 0\)</span>，使得<span class="math inline">\(y_i-(\boldsymbol{x}_i\boldsymbol{w}^T+b)&gt;0\)</span>时，有<span class="math inline">\(y_i-(\boldsymbol{x}_i\boldsymbol{w}^T+b)-\xi_i \le \epsilon\)</span>；<span class="math inline">\(y_i-(\boldsymbol{x}_i\boldsymbol{w}^T+b)&lt;0\)</span>时，有<span class="math inline">\((\boldsymbol{x}_i\boldsymbol{w}^T+b)-y_i-\xi_i&#39; \le \epsilon\)</span>。这样我们便可以得到如下的优化问题： <span class="math display">\[
\min_{\boldsymbol{w},b}~\sum_{i=1}^{n}(\xi_i+\xi_i&#39;)+\lambda||\boldsymbol{w}||^2 \\
\begin{aligned}
s.t.~&amp;y_i-(\boldsymbol{x}_i\boldsymbol{w}^T+b)-\xi_i \le \epsilon \\
&amp;(\boldsymbol{x}_i\boldsymbol{w}^T+b)-y_i-\xi_i&#39; \le \epsilon \\
&amp;\xi_i\ge 0 \\
&amp;\xi_i&#39;\ge 0
\end{aligned}
\]</span> 上述的优化问题可以改写为等价问题： <span class="math display">\[
\min_{\boldsymbol{w},b}~C\sum_{i=1}^{n}(\xi_i+\xi_i&#39;)+\frac{1}{2}||\boldsymbol{w}||^2 \\
\begin{aligned}
s.t.~&amp;y_i-(\boldsymbol{x}_i\boldsymbol{w}^T+b)-\xi_i \le \epsilon \\
&amp;(\boldsymbol{x}_i\boldsymbol{w}^T+b)-y_i-\xi_i&#39; \le \epsilon \\
&amp;\xi_i\ge 0 \\
&amp;\xi_i&#39;\ge 0
\end{aligned}
\]</span></p>
<h2 id="求解优化问题-2">求解优化问题</h2>
<p>为了求解这一问题，我们构造拉格朗日函数： <span class="math display">\[
\mathcal{L}(\boldsymbol{w},b,\alpha_i,\alpha_i&#39;,\beta_i,\beta_i&#39;,\xi_i,\xi_i&#39;)=\frac{1}{2}||\boldsymbol{w}||^2+C\sum_{i=1}^{n}(\xi_i+\xi_i&#39;)-\sum_{i=1}^{n}\alpha_i(\xi_i+\epsilon-y_i+\boldsymbol{x}_i\boldsymbol{w}^T+b)-\sum_{i=1}^{n}\alpha_i&#39;(y_i-\boldsymbol{x}_i\boldsymbol{w}^T-b+\epsilon+\xi_i&#39;)-\sum_{i=1}^{n}\beta_i \xi_i-\sum_{i=1}^{n}\beta_i \xi_i&#39; \\
\begin{aligned}
s.t.~&amp;\alpha_i\ge 0\\
&amp;\alpha_i&#39;\ge 0\\
&amp;\beta_i\ge 0 \\
&amp;\beta_i&#39;\ge 0
\end{aligned}
\]</span></p>
<p>因此，我们的优化目标为： <span class="math display">\[
\min_{\boldsymbol{w},b,\xi_i}\max_{\alpha_i \ge 0,\alpha_i&#39;\ge 0,\beta_i\ge 0}\mathcal{L}(\boldsymbol{w},b,\alpha_i,\alpha_i&#39;,\beta_i,\beta_i&#39;,\xi_i,\xi_i&#39;)
\]</span> 类似地，这一优化问题满足KKT条件与Slater条件，因此可以通过求解如下的对偶问题，来获得原始问题的解： <span class="math display">\[
\max_{\alpha_i \ge 0,\alpha_i&#39;\ge 0,\beta_i \ge 0,\beta_i&#39; \ge 0}\min_{\boldsymbol{w},b,\xi_i,\xi_i&#39;}\mathcal{L}(\boldsymbol{w},b,\alpha_i,\alpha_i&#39;,\beta_i,\beta_i&#39;,\xi_i,\xi_i&#39;)
\]</span> 对于上述的对偶问题，我们首先求解<span class="math inline">\(\min_{\boldsymbol{w},b,\xi_i,\xi_i&#39;}\mathcal{L}(\boldsymbol{w},b,\alpha_i,\alpha_i&#39;,\beta_i,\beta_i&#39;,\xi_i,\xi_i&#39;)\)</span>。分别对<span class="math inline">\(\boldsymbol{w}\)</span>，<span class="math inline">\(b\)</span>，<span class="math inline">\(\xi_i\)</span>和<span class="math inline">\(\xi_i&#39;\)</span>求偏导，可得： <span class="math display">\[
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial \boldsymbol{w}}&amp;=\boldsymbol{w}-\sum_{i=1}^{n}\alpha_i\boldsymbol{x}_i+\sum_{i=1}^{n}\alpha_i&#39;\boldsymbol{x}_i \\
\frac{\partial \mathcal{L}}{\partial b}&amp;=\sum_{i=1}^{n}\alpha_i&#39;-\sum_{i=1}^{n}\alpha_i \\
\frac{\partial \mathcal{L}}{\partial \xi_i}&amp;=C-\alpha_i-\beta_i\\
\frac{\partial \mathcal{L}}{\partial \xi_i&#39;}&amp;=C-\alpha_i&#39;-\beta_i&#39;
\end{aligned}
\]</span> 令上述四个偏导数的值为0，我们可得： <span class="math display">\[
\boldsymbol{w}=\sum_{i=1}^{n}(\alpha_i&#39;-\alpha_i)\boldsymbol{x}_i \\
\sum_{i=1}^{n}(\alpha_i&#39;-\alpha_i)=0\\
\alpha_i+\beta_i=C \\
\alpha_i&#39;+\beta_i&#39;=C
\]</span> 将上述四式代入到<span class="math inline">\(\min_{\boldsymbol{w},b,\xi_i,\xi_i&#39;}\mathcal{L}(\boldsymbol{w},b,\alpha_i,\alpha_i&#39;,\beta_i,\beta_i&#39;,\xi_i,\xi_i&#39;)\)</span>的表达式中，可得： <span class="math display">\[
\min_{\boldsymbol{w},b,\xi_i,\xi_i&#39;}\mathcal{L}(\boldsymbol{w},b,\alpha_i,\alpha_i&#39;,\beta_i,\beta_i&#39;,\xi_i,\xi_i&#39;)=-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}(\alpha_i&#39;-\alpha_i)(\alpha_j&#39;-\alpha_j)\boldsymbol{x}_i\boldsymbol{x}_j-\sum_{i=1}^{n}(\alpha_i&#39;-\alpha_i)y_i-\sum_{i=1}^{n}(\alpha_i&#39;+\alpha_i)\epsilon \\
\begin{aligned}
s.t.~&amp;\sum_{i=1}^{n}(\alpha_i&#39;-\alpha_i)=0\\
&amp;0\le \alpha_i\le C \\
&amp;0\le \alpha_i&#39;\le C
\end{aligned}
\]</span> 之后便可以使用SMO算法进行求解。</p>
<h1 id="模型求解smo算法">模型求解—SMO算法</h1>
<h2 id="迭代公式的导出">迭代公式的导出</h2>
<p>SMO算法来自于论文《Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines》。这一算法的思想是每次从所有的<span class="math inline">\(\alpha_i\)</span>中随机抽取出任意两个<span class="math inline">\(\alpha_1\)</span>和<span class="math inline">\(\alpha_2\)</span>，固定其它所有的<span class="math inline">\(\alpha_i\)</span>，使得目标函数只是关于<span class="math inline">\(\alpha_1\)</span>和<span class="math inline">\(\alpha_2\)</span>的函数，然后求解这一子问题。通过这样不断地迭代求解子问题，从而最终达到求解原问题的目的。因此需要注意，在下面的推导过程中，<span class="math inline">\(\alpha_1\)</span>和<span class="math inline">\(\alpha_2\)</span>可以对应于任意两个约束条件的乘子。</p>
<p>回顾SVM最终需要优化的问题： <span class="math display">\[
\max_{\alpha_i}~\sum_{i=1}^{n}\alpha_i-\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i\boldsymbol{x}_j^T \\
\]</span> 按照SMO算法的思路，子问题的目标函数可以写作： <span class="math display">\[
\Psi(\alpha_1,\alpha_2)=\alpha_1+\alpha_2-\frac{1}{2}K_{11}\alpha_1^2-\frac{1}{2}K_{22}\alpha_2^2-y_1y_2K_{12}\alpha_1\alpha_2-v_1y_1\alpha_1-v_2y_2\alpha_2+\text{const}
\]</span></p>
<p>其中： <span class="math display">\[
K_{ij}=\boldsymbol{x}_i\boldsymbol{x}_j^T \\
v_i=\sum_{j=3}^{n}y_j\alpha_jK_{ij}
\]</span></p>
<p>令<span class="math inline">\(f(\boldsymbol{x})=\boldsymbol{x}\boldsymbol{w}^T+b=\sum_{i=1}^{n}y_i\alpha_i\boldsymbol{x}\boldsymbol{x}_i^T+b\)</span>，因此<span class="math inline">\(v_i\)</span>也可以记作如下的格式（这在后面的推导中将会用到）： <span class="math display">\[
v_i=f(\boldsymbol{x}_i)-y_1\alpha_1K_{1i}-y_2\alpha_2K_{2i}-b
\]</span> 假设选取的两个乘子<span class="math inline">\(\alpha_1\)</span>和<span class="math inline">\(\alpha_2\)</span>在更新之前的值为<span class="math inline">\(\alpha_1^{old}\)</span>和<span class="math inline">\(\alpha_2^{old}\)</span>，更新之后为<span class="math inline">\(\alpha_1^{new}\)</span>和<span class="math inline">\(\alpha_2^{new}\)</span>，那么它们更新前后满足如下的约束关系： <span class="math display">\[
\alpha_1^{old}y_1+\alpha_2^{old}y_2=\alpha_1^{new}y_1+\alpha_2^{new}y_2=\zeta
\]</span> 其中<span class="math inline">\(\zeta\)</span>为一个常数，<span class="math inline">\(\zeta=-\sum_{i=3}^{n}\alpha_iy_i\)</span>。</p>
<p>由于同时求解两个乘子较困难，因此可以先计算<span class="math inline">\(\alpha_2^{new}\)</span>，然后再通过<span class="math inline">\(\alpha_2^{new}\)</span>来表示<span class="math inline">\(\alpha_1^{new}\)</span>。</p>
<p>首先我们可以根据约束条件<span class="math inline">\(\sum_{i=1}^{n}\alpha_iy_i=0\)</span>，消掉<span class="math inline">\(\Psi(\alpha_1,\alpha_2)\)</span>中的<span class="math inline">\(\alpha_1\)</span>。在关系式<span class="math inline">\(\alpha_1y_1+\alpha_2y_2=\zeta\)</span>的两端同时乘上<span class="math inline">\(y_1\)</span>，可得： <span class="math display">\[
\alpha_1=y_1\zeta-y_1y_2\alpha_2
\]</span> 将上式代入到<span class="math inline">\(\Psi(\alpha_1,\alpha_2)\)</span>的表达式中，<span class="math inline">\(\Psi\)</span>就变成了关于<span class="math inline">\(\alpha_2\)</span>的一元函数： <span class="math display">\[
\Psi(\alpha_2)=y_1\zeta-y_1y_2\alpha_2+\alpha_2-\frac{1}{2}K_{11}(y_1\zeta-y_1y_2\alpha_2)^2-\frac{1}{2}K_{22}\alpha_2^2-K_{12}(y_2\zeta-\alpha_2)\alpha_2-v_1(\zeta-y_2\alpha_2)-v_2y_2\alpha_2+\text{const}
\]</span> 接下来我们对<span class="math inline">\(\alpha_2\)</span>求导，可得： <span class="math display">\[
\frac{d\Psi}{d\alpha_2}=-y_1y_2+1+K_{11}(y_2\zeta-\alpha_2)-K_{22}\alpha_2-K_{12}(y_2\zeta-2\alpha_2)+v_1y_2-v_2y_2
\]</span> 令<span class="math inline">\(\frac{d\Psi}{d\alpha_2}=0\)</span>，并将上式中的<span class="math inline">\(1\)</span>替换为<span class="math inline">\(y_2y_2\)</span>，便可以得到求解<span class="math inline">\(\alpha_2^{new}\)</span>的表达式： <span class="math display">\[
(K_{11}+K_{22}-2K_{12})\alpha_2^{new}=(K_{11}-K_{12})y_2\zeta+(v_1^{old}-v_2^{old}+y_2-y_1)y_2
\]</span> 在上式中，由于每一次迭代计算时选取的<span class="math inline">\(\alpha_1\)</span>和<span class="math inline">\(\alpha_2\)</span>都不同，这就要求每次迭代的时候都要计算一次<span class="math inline">\(v_1\)</span>、<span class="math inline">\(v_2\)</span>的值，增加了不少的计算开支。因此，我们考虑对<span class="math inline">\(v_1-v_2\)</span>进行化简，可得： <span class="math display">\[
\begin{aligned}
v_1-v_2=&amp;f(\boldsymbol{x}_1)-y_1\alpha_1K_{11}-y_2\alpha_2K_{21}-f(\boldsymbol{x}_2)+y_1\alpha_1K_{12}+y_2\alpha_2K_{22} \\
=&amp;f(\boldsymbol{x}_1)-f(\boldsymbol{x}_2)+y_1\alpha_1(K_{12}-K_{11})+y_2\alpha_2(K_{22}-K_{12}) \\
=&amp;f(\boldsymbol{x}_1)-f(\boldsymbol{x}_2)+y_1(y_1\zeta-y_1y_2\alpha_2)(K_{12}-K_{11})+y_2\alpha_2(K_{22}-K_{12}) \\
=&amp;f(\boldsymbol{x}_1)-f(\boldsymbol{x}_2)+\zeta(K_{12}-K_{11})+y_2\alpha_2(K_{11}+K_{22}-2K_{12})
\end{aligned}
\]</span> 将其代入到求解<span class="math inline">\(\alpha_2^{new}\)</span>的表达式中，可得： <span class="math display">\[
(K_{11}+K_{22}-2K_{12})\alpha_2^{new}=(K_{11}+K_{22}-2K_{12})\alpha_2^{old}+[f(\boldsymbol{x}_1)-f(\boldsymbol{x}_2)+y_2-y_1]y_2
\]</span> 在上式中，我们记<span class="math inline">\(\eta=(K_{11}+K_{22}-2K_{12})\)</span>，<span class="math inline">\(e_i=f(\boldsymbol{x}_i)-y_i=\sum_{j=1}^{n}y_j\alpha_j\boldsymbol{x}_i\boldsymbol{x}_j^T+b-y_i\)</span>，便可得到计算<span class="math inline">\(\alpha_2^{new}\)</span>的表达式： <span class="math display">\[
\alpha_2^{new}=\alpha_2^{old}+\frac{(e_1-e_2)y_2}{\eta}
\]</span> 这样，我们便可通过<span class="math inline">\(\alpha_2^{old}\)</span>计算出<span class="math inline">\(\alpha_2^{new}\)</span>，而<span class="math inline">\(\alpha_1^{new}\)</span>可以通过<span class="math inline">\(\alpha_1^{new}=\alpha_1^{old}+(\alpha_2^{old}-\alpha_2^{new})y_1y_2\)</span>来计算。</p>
<h2 id="迭代结果的修剪">迭代结果的修剪</h2>
<p>上面得出的迭代公式可以直接用来迭代求解硬间隔SVM的<span class="math inline">\(\alpha_i\)</span>。但是对于软间隔SVM来说，由于限制条件<span class="math inline">\(0\le \alpha_i \le C\)</span>的加入，就要求我们在迭代求解的每一步都需要对计算结果进行修剪。同时需要注意的是，在计算<span class="math inline">\(\alpha_1^{new}\)</span>时，需要先对<span class="math inline">\(\alpha_2^{new}\)</span>的结果进行修剪之后再计算。</p>
<p>首先我们确定<span class="math inline">\(\alpha_2^{new}\)</span>的取值范围，假设它的取值范围是<span class="math inline">\(L\le \alpha_2^{new}\le H\)</span>，我们分不同情况来讨论<span class="math inline">\(L\)</span>和<span class="math inline">\(H\)</span>的取值：</p>
<ul>
<li><span class="math inline">\(y_1\ne y_2\)</span>：此时有<span class="math inline">\(\alpha_1^{new}-\alpha_2^{new}=\zeta\)</span>或<span class="math inline">\(\alpha_2^{new}-\alpha_1^{new}=\zeta\)</span>，结合约束条件<span class="math inline">\(0&lt;\alpha_1^{new}&lt;C\)</span>与<span class="math inline">\(0&lt;\alpha_2^{new}&lt;C\)</span>，可得，
<ul>
<li>当<span class="math inline">\(\alpha_1^{new}-\alpha_2^{new}=\zeta\)</span>时，<span class="math inline">\(L=\max(0,-\zeta)\)</span>，<span class="math inline">\(H=\min(C-\zeta,C)\)</span>。</li>
<li>当<span class="math inline">\(\alpha_2^{new}-\alpha_1^{new}=\zeta\)</span>时，<span class="math inline">\(L=\max(0,\zeta)\)</span>，<span class="math inline">\(H=\min(C+\zeta,C)\)</span>。</li>
</ul></li>
<li><span class="math inline">\(y_1=y_2\)</span>：此时有<span class="math inline">\(\alpha_1^{new}+\alpha_2^{new}=\zeta\)</span>，结合约束条件<span class="math inline">\(0&lt;\alpha_1^{new}&lt;C\)</span>与<span class="math inline">\(0&lt;\alpha_2^{new}&lt;C\)</span>可得<span class="math inline">\(L=\max(0,\zeta-C)\)</span>，<span class="math inline">\(H=\min(C,\zeta)\)</span></li>
</ul>
<p>可以用下图来标识上述的两种情况：</p>
<figure>
<img data-src="https://raw.githubusercontent.com/lyf35/blog_figures/main/img/20210301213535" alt="这里写图片描述" /><figcaption aria-hidden="true">这里写图片描述</figcaption>
</figure>
<p>接下来，我们便可以根据所确定的<span class="math inline">\(L\)</span>与<span class="math inline">\(R\)</span>的值来对<span class="math inline">\(\alpha_2^{new,unclipped}\)</span>的值进行修剪： <span class="math display">\[
\alpha_2^{new}=
\begin{cases}
\begin{aligned}
L~~~~~~~~~~~~~~~~~~~~~&amp;\alpha_2^{new,unclipped}&lt;L \\
\alpha_2^{new,unclipped}~~~&amp;L\le \alpha_2^{new,unclipped}\le H \\
H~~~~~~~~~~~~~~~~~~~~&amp; H&lt;\alpha_2^{new,unclipped}
\end{aligned}
\end{cases}
\]</span></p>
<h2 id="临界情况的处理">临界情况的处理</h2>
<p>大部分情况下都有<span class="math inline">\(\eta&gt;0\)</span>成立，但是当<span class="math inline">\(\boldsymbol{x}_1\)</span>与<span class="math inline">\(\boldsymbol{x}_2\)</span>共线的时候，<span class="math inline">\(\eta=0\)</span>；如果使用了不满足Mercer定理的核函数（即使用核函数<span class="math inline">\(\kappa\)</span>将运算<span class="math inline">\(\boldsymbol{x}_i\boldsymbol{x}_j^T\)</span>替换为<span class="math inline">\(\kappa(\boldsymbol{x}_i,\boldsymbol{x}_j)\)</span>），会出现<span class="math inline">\(\eta&lt;0\)</span>的情况。对于这些情况，需要做一些特殊的处理。</p>
<p>此时，分别取<span class="math inline">\(\alpha_2^{new}=L\)</span>和<span class="math inline">\(\alpha_2^{new}=H\)</span>，计算出相应的<span class="math inline">\(\alpha_1^{new}\)</span>，然后将其代入到<span class="math inline">\(\Psi(\alpha_1,\alpha_2)\)</span>中，取<span class="math inline">\(\Psi\)</span>较大的一组作为<span class="math inline">\(\alpha_1^{new}\)</span>和<span class="math inline">\(\alpha_2^{new}\)</span>的值。</p>
<h2 id="偏置的更新">偏置的更新</h2>
<p>在更新完<span class="math inline">\(\alpha_1\)</span>和<span class="math inline">\(\alpha_2\)</span>的值之后，我们还需要更新偏置<span class="math inline">\(b\)</span>的值，它关系到<span class="math inline">\(f(\boldsymbol{x})\)</span>的计算，从而影响到<span class="math inline">\(e_i\)</span>的计算。</p>
<p><span class="math inline">\(b\)</span>的更新方式可以分为如下三种情况：</p>
<ol type="1">
<li>如果在硬间隔SVM中<span class="math inline">\(\alpha_1&gt;0\)</span>，软间隔SVM中<span class="math inline">\(0&lt;\alpha_1&lt;C\)</span>，此时<span class="math inline">\(y_1(\boldsymbol{x}_1\boldsymbol{w}^T+b)=1\)</span>成立，据此可以算出更新后的<span class="math inline">\(b\)</span>值。</li>
<li>如果在硬间隔SVM中<span class="math inline">\(\alpha_2&gt;0\)</span>，软间隔SVM中<span class="math inline">\(0&lt;\alpha_2&lt;C\)</span>，此时<span class="math inline">\(y_2(\boldsymbol{x}_2\boldsymbol{w}^T+b)=1\)</span>成立，据此可以算出更新后的<span class="math inline">\(b\)</span>值。需要说明的是，如果1和2同时满足，它们计算出的<span class="math inline">\(b\)</span>值应该是相等的。</li>
<li>以上都不满足，此时按照<span class="math inline">\(y_1(\boldsymbol{x}_1\boldsymbol{w}^T+b)=1\)</span>和<span class="math inline">\(y_2(\boldsymbol{x}_2\boldsymbol{w}^T+b)=1\)</span>可以分别算出<span class="math inline">\(b_1\)</span>和<span class="math inline">\(b_2\)</span>，最终取<span class="math inline">\(b=\frac{1}{2}(b_1+b_2)\)</span>。</li>
</ol>
<p>按照上述的分析，可以将<span class="math inline">\(b\)</span>的更新方法总结如下： <span class="math display">\[
b^{new}=
\begin{cases}
\begin{aligned}
b_1~~~~~~~~~~~&amp;if~~0&lt;\alpha_1&lt;C\\
b_2~~~~~~~~~~~&amp;else~if~~0&lt;\alpha_2&lt;C\\
\frac{b_1+b_2}{2}~~&amp;else
\end{aligned}
\end{cases}
\]</span> 其中， <span class="math display">\[
b_1=-e_1-y_1K_{11}(\alpha_1^{new}-\alpha_1^{old})-y_2K_{12}(\alpha_2^{new}-\alpha_2^{old})+b^{old} \\
b_2=-e_2-y_1K_{12}(\alpha_1^{new}-\alpha_1^{old})-y_2K_{22}(\alpha_2^{new}-\alpha_2^{old})+b^{old}
\]</span></p>
<h2 id="变量的选择">变量的选择</h2>
<p>上述分析是在<span class="math inline">\(N\)</span>个<span class="math inline">\(\alpha_i\)</span>中随机选出两个变量进行优化的方法，接下来的问题便是考虑如何选择<span class="math inline">\(\alpha_1\)</span>和<span class="math inline">\(\alpha_2\)</span>（其实也就是选择它们相对应的样本点），从而使得目标函数下降地最快。</p>
<p>具体来说，第一个变量<span class="math inline">\(\alpha_1\)</span>的选择被称为外循环，它是违反KKT条件最严重的；而另外一个变量<span class="math inline">\(\alpha_2\)</span>的选择被称为内循环，它的选择希望能够在优化过程中使得<span class="math inline">\(\alpha_2\)</span>有较大的变化。</p>
<blockquote>
<p>备注：</p>
<p>以软间隔SVM为例，根据KKT条件，SVM在优化过程中所对应的<span class="math inline">\(\alpha_i\)</span>的含义如下：</p>
<ul>
<li><span class="math inline">\(\alpha_i=0\)</span>：<span class="math inline">\(y_i(\boldsymbol{x}_i\boldsymbol{w}^T+b)\ge 1\)</span>，代表样本点被正确分类或是在间隔边界上</li>
<li><span class="math inline">\(0&lt;\alpha_i&lt;C\)</span>：<span class="math inline">\(y_i(\boldsymbol{x}_i\boldsymbol{w}^T+b)=1\)</span>，代表样本点在间隔边界上</li>
<li><span class="math inline">\(\alpha_i=C\)</span>：<span class="math inline">\(y_i(\boldsymbol{x}_i\boldsymbol{w}^T+b)\le 1\)</span>（因此时<span class="math inline">\(\beta_i=0\)</span>），样本点可能在间隔边界上，可能在间隔边界之间，也可能被错误分类</li>
</ul>
<p>违反KKT条件的意思便是上述的对应关系不满足。</p>
<p>但是需要说明的是，满足条件<span class="math inline">\(\alpha_i&gt;0\)</span>所对应的那些点都被称为支持向量（此处参考《统计学习方法》中的叙述）。</p>
</blockquote>
<h1 id="优缺点">优缺点</h1>
<p>优点：</p>
<ol type="1">
<li>SVM 的最终决策函数只由少数的支持向量所确定，计算的复杂性取决于支持向量的数目，而不是样本空间的维数，这在某种意义上避免了维数灾难；</li>
<li>少数支持向量决定了最终结果，这不但可以剔除大量冗余样本，而且具有较好的鲁棒性。</li>
</ol>
<p>缺点：</p>
<ol type="1">
<li>SVM算法对大规模训练样本的训练较慢；</li>
<li>SVM本身只能做二分类，如果要解决多分类问题则需要构造多个模型；</li>
<li>对参数和核函数的选择敏感</li>
</ol>
<h1 id="代码示例">代码示例</h1>
<h2 id="分类">分类</h2>
<p>我们使用Sklearn自带的Iris数据集，来演示如何用sklearn中的支持向量机分类器做分类任务。</p>
<p>Iris数据集的四个特征分别为：sepal length (cm)、sepal width (cm)、petal length (cm)、petal width (cm)，三种不同的类别0、1、2分别对应于Iris-Setosa、Iris-Versicolour和Iris-Virginica这三类。</p>
<p>为了方便做可视化，我们对数据进行精简，只选取2、3两列来训练与预测模型。我们取petal length (cm)和petal width (cm)这两个特征来训练和预测。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC,LinearSVC</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> KBinsDiscretizer</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X,y=load_iris(return_X_y=<span class="literal">True</span>)</span><br><span class="line">X=X[:,<span class="number">2</span>:<span class="number">4</span>]</span><br><span class="line">train_x,test_x,train_y,test_y=train_test_split(X,y,test_size=<span class="number">0.3</span>)</span><br></pre></td></tr></table></figure>
<p>支持向量机分类器<code>SVC</code>的主要参数包括：</p>
<ul>
<li>C：惩罚因子，C越大则对误分类的惩罚越大</li>
<li>kernel：代表使用的核函数，默认为高斯核</li>
<li>degree：使用多项式核时，多项式的阶数，默认为3</li>
<li>gamma：默认使用'scale'，代表自动选取；也可以自己传入数值。这一参数适用于rbf、poly和sigmoid三个核函数</li>
<li>coef0：核函数中的独立参数，适用于poly和sigmoid核函数</li>
</ul>
<p>此外，Sklearn还提供了<code>LinearSVC</code>线性分类器，与<code>SVC</code>相比，它默认使用线性核，默认的损失函数为squared_hinge。它的重要参数包括：</p>
<ul>
<li>penalty：正则项，默认为l2，也可以改为l1</li>
<li>loss：损失函数，默认为squared_hinge，也可以改为hinge</li>
<li>C：对于误分类样本的惩罚因子，C越大则对误分类的惩罚越大</li>
</ul>
<p>由于二者的用法类似，因此我们以<code>SVC</code>为例来展示其用法。同时为了比较不同核函数的区别，我们分别作图展示在使用linear、poly、rbf三种核函数时，其得到的分类边界。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_mesh,Y_mesh=np.meshgrid(np.linspace(<span class="number">0</span>,<span class="number">8</span>,<span class="number">200</span>),np.linspace(<span class="number">0</span>,<span class="number">5</span>,<span class="number">100</span>))</span><br><span class="line">XY_mesh=np.concatenate([X_mesh.reshape(-<span class="number">1</span>,<span class="number">1</span>),Y_mesh.reshape(-<span class="number">1</span>,<span class="number">1</span>)],axis=<span class="number">1</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">kernels=[<span class="string">&#x27;linear&#x27;</span>,<span class="string">&#x27;rbf&#x27;</span>,<span class="string">&#x27;poly&#x27;</span>]</span><br><span class="line">fig=plt.figure(figsize=(<span class="number">15</span>,<span class="number">3</span>))</span><br><span class="line">axes=fig.subplots(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    clf=SVC(kernel=kernels[i],gamma=<span class="number">2</span>)</span><br><span class="line">    clf.fit(train_x,train_y)</span><br><span class="line">    mesh=clf.predict(XY_mesh)</span><br><span class="line">    axes[i].set_title(<span class="string">&#x27;kernel function: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(kernels[i]))</span><br><span class="line">    axes[i].set_xlabel(<span class="string">&#x27;sepal length (cm)&#x27;</span>)</span><br><span class="line">    axes[i].set_ylabel(<span class="string">&#x27;sepal width (cm)&#x27;</span>)</span><br><span class="line">    contour=axes[i].contourf(X_mesh,Y_mesh,mesh.reshape(X_mesh.shape),cmap=<span class="string">&#x27;Greys&#x27;</span>)</span><br><span class="line">    fig.colorbar(contour,ax=axes[i])</span><br><span class="line">    axes[i].scatter(test_x[test_y==<span class="number">0</span>][:,<span class="number">0</span>],test_x[test_y==<span class="number">0</span>][:,<span class="number">1</span>],label=<span class="string">&#x27;Iris-Setosa&#x27;</span>)</span><br><span class="line">    axes[i].scatter(test_x[test_y==<span class="number">1</span>][:,<span class="number">0</span>],test_x[test_y==<span class="number">1</span>][:,<span class="number">1</span>],label=<span class="string">&#x27;Iris-Versicolour&#x27;</span>)</span><br><span class="line">    axes[i].scatter(test_x[test_y==<span class="number">2</span>][:,<span class="number">0</span>],test_x[test_y==<span class="number">2</span>][:,<span class="number">1</span>],label=<span class="string">&#x27;Iris-Virginica&#x27;</span>)</span><br><span class="line">    axes[i].legend()</span><br></pre></td></tr></table></figure>
<figure>
<img data-src="https://raw.githubusercontent.com/lyf35/blog_figures/main/img/20210402080210.png" alt="SVC示例" /><figcaption aria-hidden="true">SVC示例</figcaption>
</figure>
<h2 id="回归">回归</h2>
<p>我们使用Sklearn自带的Diabetes数据集，来演示如何用sklearn中的SVM做回归任务。</p>
<p>这一数据集共有10个特征，且这些特征都已经被标准化至<span class="math inline">\((-2,2)\)</span>区间内，待预测的值是25-346范围内的整数。由于我们的目的主要是说明模型的用法，因此省略了特征工程的步骤，将这些数据直接用来训练模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_diabetes</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVR, SVR</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X,y=load_diabetes(return_X_y=<span class="literal">True</span>)</span><br><span class="line">train_x,test_x,train_y,test_y=train_test_split(X,y,test_size=<span class="number">0.3</span>)</span><br></pre></td></tr></table></figure>
<p><code>LinearSVR</code>回归器的主要参数有：</p>
<ul>
<li>epsilon：代表epsilon-insensitive损失函数中的epsilon参数，相当于是误差在0-epsilon范围内都会被记作0。默认值为0</li>
<li>C：正则参数，数值越大则对于模型参数的惩罚力度越小。默认值为1.0</li>
<li>loss：模型的损失函数，可以选择'epsilon-insensitive'或者'squared_epsilon-insensitive'。默认为'epsilon-insensitive'</li>
</ul>
<p><code>SVR</code>回归器的主要参数有：</p>
<ul>
<li>kernel：模型使用的核函数，可以选择'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'，或是自己传入一个函数。默认为'rbf'</li>
<li>degree：使用'poly'核时多项式的系数。默认为3</li>
<li>gamma：使用'rbf', 'poly'和'sigmoid'三种核函数时，核函数中的gamma参数。默认为'scale'，也可以传入'auto'，或者是一个浮点数</li>
<li>coef0：使用'poly'和'sigmoid'核函数时，核函数中的独立项。默认为0</li>
<li>epsilon：代表epsilon-insensitive损失函数中的epsilon参数，相当于是误差在0-epsilon范围内都会被记作0。默认值为0.1</li>
<li>C：正则参数，数值越大则对于模型参数的惩罚力度越小。默认值为1.0</li>
</ul>
<p><code>LinearSVR</code>相当于是<code>SVR</code>在使用线性核时的特殊情况，但是<code>SVR</code>默认使用epsilon-insensitive损失函数且不能修改。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">reg=SVR()</span><br><span class="line">reg.fit(train_x,train_y)</span><br><span class="line">pred=reg.predict(test_x)</span><br><span class="line">errors=pred-test_y</span><br></pre></td></tr></table></figure>
<h1 id="附广义约束优化问题">附：广义约束优化问题</h1>
<h2 id="拉格朗日乘子法与kkt条件">拉格朗日乘子法与KKT条件</h2>
<p>给定如下的约束优化问题 <span class="math display">\[
\max_{\boldsymbol{x}}f(\boldsymbol{x}) \\
\begin{aligned}
s.t. ~&amp;g_i(\boldsymbol{x})\le0,i=1,2,\dots,p \\
&amp;h_j(\boldsymbol{x})=0,j=1,2,\dots,q
\end{aligned}
\]</span> 也就是说，函数<span class="math inline">\(f(\boldsymbol{x})\)</span>的自变量为一个<span class="math inline">\(n\)</span>维向量，要最大化的目标函数<span class="math inline">\(f(\boldsymbol{x})\)</span>满足若干的等式和不等式约束（如果优化目标是求<span class="math inline">\(\min_{\boldsymbol{x}}f(\boldsymbol{x})\)</span>，则将约束条件中的不等关系改为<span class="math inline">\(g_i(\boldsymbol{x})\ge 0\)</span>即可，下面的内容中不再赘述）。我们可以构造如下的拉格朗日函数： <span class="math display">\[
\mathcal{L}(\boldsymbol{x},\mu,\lambda)=f(\boldsymbol{x})-\sum_{i}\mu_i g_i(\boldsymbol{x})-\sum_{j}\lambda_j h_j(\boldsymbol{x}) \\
\]</span> 其中，<span class="math inline">\(\mu_i\ge 0\)</span>，<span class="math inline">\(\lambda_j\in R\)</span>为拉格朗日乘子。</p>
<p>对于不等式约束，只要满足KKT条件，则依然可以使用拉格朗日乘子法解决。KKT条件是指，如果有一个点<span class="math inline">\(\boldsymbol{x}^*\)</span>是满足上述所有约束的极值点，则如下关系成立： <span class="math display">\[
\nabla f(\boldsymbol{x}^*)-\sum_{i}\mu_i\nabla g_i(\boldsymbol{x}^*)-\sum_{j}\lambda_j\nabla h_j(\boldsymbol{x}^*)=0 \\
\begin{aligned}
s.t.~ &amp;\mu_i\ge 0 \\
&amp;\mu_i g_i(\boldsymbol{x}^*)=0 \\
&amp;g_i(\boldsymbol{x}^*)\le 0\\
&amp;h_j(\boldsymbol{x}^*)=0
\end{aligned}
\]</span> 也就是说，在这个极值点处，拉格朗日函数的梯度为0，相当于<span class="math inline">\(f\)</span>的梯度是一系列的不等式约束<span class="math inline">\(g_i\)</span>的梯度和等式约束<span class="math inline">\(h_i\)</span>的梯度所形成的线性组合。在这个线性组合中，等式约束梯度的权值<span class="math inline">\(\lambda_j\)</span>没有要求；不等式约束梯度的权值<span class="math inline">\(\mu_i\)</span>为非负值。并且如果某个<span class="math inline">\(g_i(\boldsymbol{x}^*)\)</span>严格小于0，那么这个约束不会出现在加权式中，也就是对于优化问题没有影响；只有<span class="math inline">\(\boldsymbol{x}^*\)</span>恰好在边界<span class="math inline">\(g_i(\boldsymbol{x}^*)=0\)</span>的那些<span class="math inline">\(g_i\)</span>的梯度才对优化问题有影响，它们则会出现在加权式中。</p>
<p>如果KKT条件满足，则优化目标就变为求解拉格朗日函数<span class="math inline">\(\mathcal{L}(\boldsymbol{x},\mu,\lambda)\)</span>的最大值。</p>
<p>在上面的表述中，如果去掉不等式约束的部分，就变成了等式约束条件下的拉格朗日乘子法。</p>
<h2 id="拉格朗日对偶问题">拉格朗日对偶问题</h2>
<p>考虑如下的约束优化问题： <span class="math display">\[
\max_{\boldsymbol{x}}f(\boldsymbol{x}) \\
\begin{aligned}
s.t. ~&amp;g_i(\boldsymbol{x})\le0,i=1,2,\dots,p \\
&amp;h_j(\boldsymbol{x})=0,j=1,2,\dots,q
\end{aligned}
\]</span> 以及这一优化问题对应的拉格朗日函数： <span class="math display">\[
\mathcal{L}(\boldsymbol{x},\mu,\lambda)=f(\boldsymbol{x})-\sum_{i}\mu_i g_i(\boldsymbol{x})-\sum_{j}\lambda_j h_j(\boldsymbol{x}) \\
\]</span> 其中，<span class="math inline">\(\mu_i\ge 0\)</span>，<span class="math inline">\(\lambda_j\in R\)</span>为拉格朗日乘子。</p>
<p>对于上述的拉格朗日函数<span class="math inline">\(\mathcal{L}(\boldsymbol{x},\mu,\lambda)\)</span>，当我们固定<span class="math inline">\(\boldsymbol{x}\)</span>的值时，它满足如下关系： <span class="math display">\[
f(\boldsymbol{x})=\min_{\mu_i\ge 0,\lambda} \mathcal{L}(\boldsymbol{x},\mu,\lambda)\le \mathcal{L}(\boldsymbol{x},\mu,\lambda)
\]</span> 由于在<span class="math inline">\(\mathcal{L}(\boldsymbol{x},\mu,\lambda)\)</span>的表达式中，所有的<span class="math inline">\(h_j(\boldsymbol{x})\)</span>项都为0，同时所有的<span class="math inline">\(\mu_ig_i(\boldsymbol{x}) \le 0\)</span>恒成立，因此最小值只有在它们都取0的时候得到。反之，如果上述的任意一个约束条件不满足，则只需要令相应的乘子取无穷大，便可以取得<span class="math inline">\(f(\boldsymbol x)\rightarrow -\infty\)</span>，这样便会导致问题无解，也就是说约束条件必须满足。因此，我们便得到了上述的不等关系。</p>
<p>经过这样的转变之后，我们便将约束融合到一起，得到了如下的无约束优化目标： <span class="math display">\[
\max_{\boldsymbol{x}}f(\boldsymbol{x})=\max_{\boldsymbol{x}}\min_{\mu_i\ge 0,\lambda} \mathcal{L}(\boldsymbol{x},\mu,\lambda)
\]</span> 这一表达式与原优化问题等价，我们将其称为原始问题，并将其解记作<span class="math inline">\(\boldsymbol{p}^*\)</span>。</p>
<p>在优化理论中，每个原始问题都有一个与之对应的对偶问题（对偶问题的对偶又会变成原问题）。无论原始问题是什么形式，对偶问题总是一个凸优化的问题，这样对于那些难以求解的原始问题 （甚至是 NP 问题），均可以通过转化为对偶问题。而且当满足一定条件时，原始问题与对偶问题的解是完全等价的。</p>
<p>接下来我们说明上述原始问题的对偶问题。首先为对偶问题定义如下的对偶函数： <span class="math display">\[
D(\mu,\lambda)=\max_{\boldsymbol{x}}\mathcal{L}(\boldsymbol{x},\mu,\lambda)
\]</span> 根据这一对偶函数便可得到如下的对偶问题。它的形式与原始问题类似，只是交换了最小化和最大化的顺序： <span class="math display">\[
\min_{\mu_i\ge 0,\lambda}\max_{\boldsymbol{x}} \mathcal{L}(\boldsymbol{x},\mu,\lambda)
\]</span> 我们定义对偶问题的解为<span class="math inline">\(\boldsymbol{d}^*\)</span>，它与原始问题的解并不相同，而是满足<span class="math inline">\(\boldsymbol{d}^*\ge \boldsymbol{p}^*\)</span>。这是因为： <span class="math display">\[
D(\mu,\lambda)=\max_{\boldsymbol{x}}\mathcal{L}(\boldsymbol{x},\mu,\lambda)\ge \mathcal{L}(\boldsymbol{x},\mu,\lambda)\ge \min_{\mu_i\ge 0,\lambda} \mathcal{L}(\boldsymbol{x},\mu,\lambda)=f(\boldsymbol{x})
\]</span> 也就是说，对偶问题中的最小值比原始问题中的最大值还要大。因此，我们通过对偶性，为原始问题引入了一个下界<span class="math inline">\(\boldsymbol{d}^*\ge \boldsymbol{p}^*\)</span>。这一性质对于所有的优化问题都成立（即使原始问题非凸），被称为弱对偶性。其中，<span class="math inline">\(D(\mu,\lambda)-f(\boldsymbol{x})\)</span>被称为对偶间隔，<span class="math inline">\(\boldsymbol{d}^*-\boldsymbol{p}^*\)</span>被称作最优对偶间隔。</p>
<p>与弱对偶性相对应的一个概念为强对偶性，即满足<span class="math inline">\(\boldsymbol{d}^*=\boldsymbol{p}^*\)</span>的优化问题。在强对偶成立的情况下，可以通过求解对偶问题来获得原始问题的解。对于上述优化问题来说，如果满足KKT条件（强对偶成立的必要条件）和Slater条件（强对偶成立的充分条件），那么强对偶性成立，即存在一组<span class="math inline">\(\boldsymbol{x}^*,\mu^*,\lambda^*\)</span>使得原始问题与对偶问题的最优值相等。Slater条件是指原始问题为凸优化问题，且存在点<span class="math inline">\(\boldsymbol{x}\)</span>，使得所有的<span class="math inline">\(g_i(\boldsymbol{x})&lt;0\)</span>成立。</p>
<h1 id="参考">参考</h1>
<ol type="1">
<li>统计学习方法，李航</li>
<li>机器学习，周志华</li>
<li>https://zhuanlan.zhihu.com/p/49331510</li>
<li>https://blog.csdn.net/v_JULY_v/article/details/7624837</li>
<li>https://blog.csdn.net/u013019431/article/details/79952483</li>
<li>https://www.zhihu.com/question/23311674</li>
<li>https://www.cnblogs.com/xinchen1111/p/8804858.html</li>
<li>https://blog.csdn.net/blackyuanc/article/details/67640844</li>
<li>https://www.zhihu.com/question/58584814</li>
<li>https://www.cnblogs.com/ooon/p/5721119.html</li>
<li>https://www.cnblogs.com/ooon/p/5723725.html</li>
<li>https://blog.csdn.net/weixin_44273380/article/details/109034549</li>
<li>https://blog.csdn.net/weixin_44037337/article/details/109804487</li>
<li>https://blog.csdn.net/qq_32742009/article/details/81435141</li>
<li>https://blog.csdn.net/weixin_44508906/article/details/89979684</li>
<li>https://zhuanlan.zhihu.com/p/29212107</li>
<li>https://blog.csdn.net/luoshixian099/article/details/51227754</li>
<li>https://blog.csdn.net/qq_38734403/article/details/80442535</li>
</ol>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Yufei Luo
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://lyf35.github.io/2021/02/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" title="机器学习-支持向量机">http://lyf35.github.io/2021/02/09/机器学习-支持向量机/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</span> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 机器学习</a>
              <a href="/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 监督学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/02/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="prev" title="深度学习-循环神经网络">
      <i class="fa fa-chevron-left"></i> 深度学习-循环神经网络
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/02/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%A0%B8%E6%96%B9%E6%B3%95/" rel="next" title="机器学习-核方法">
      机器学习-核方法 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



        </div>
        
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2020 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yufei Luo</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">1.5m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">22:56</span>
</div>
  <div class="powered-by">由 <span class="exturl theme-link" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl theme-link" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZy9waXNjZXMv">NexT.Pisces</span> 强力驱动
  </div>

        






<script data-pjax>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      const visitors = document.querySelector('.leancloud_visitors');
      const url = decodeURI(visitors.id);
      const title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            const counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              Counter('post', '/classes/Counter', { title, url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      const visitors = document.querySelectorAll('.leancloud_visitors');
      const entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"ypNNXSdcGIIoldC5BBmViS2g-gzGzoHsz","app_key":"0TDwohU7eO4yHTn4tley8PtE","server_url":"https://leancloud.cn/dashboard/data.html?appid=ypNNXSdcGIIoldC5BBmViS2g-gzGzoHsz#/","security":false};
    function fetchData(api_server) {
      const Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    const api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.6" zIndex="-1" src="//cdn.jsdelivr.net/npm/ribbon.js@1/dist/ribbon.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/next-theme/pjax@0/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '.page-configurations',
    '.main-inner',
    '.post-toc-wrap',
    '.languages',
    '.pjax'
  ],
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

document.addEventListener('pjax:success', () => {
  pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  const hasTOC = document.querySelector('.post-toc');
  document.querySelector('.sidebar-inner').classList.toggle('sidebar-nav-active', hasTOC);
  document.querySelector(hasTOC ? '.sidebar-nav-toc' : '.sidebar-nav-overview').click();
  NexT.utils.updateSidebarPosition();
});
</script>


  
  <script data-pjax>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  
<script src="/js/local-search.js"></script>









<script data-pjax>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>


<script data-pjax>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.init({
      theme    : 'forest',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    }, '.mermaid');
  }, window.mermaid);
}
</script>


    <div class="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      const script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments('#gitalk-container', () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '4dcdc02d24075b0e9d3a',
      clientSecret: '53cf2cb8b89b9d4bba922bb19bda1290f0d0bf95',
      repo        : 'lyf35.github.io',
      owner       : 'lyf35',
      admin       : ['lyf35'],
      id          : '9ef8fccfd4d79e9eb5094d218809cce9',
        language: 'zh-CN',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

    </div>
</body>
</html>
